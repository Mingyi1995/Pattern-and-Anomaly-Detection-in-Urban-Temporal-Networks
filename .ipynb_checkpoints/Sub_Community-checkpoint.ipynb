{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add necessary libraries\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.mixture import GaussianMixture \n",
    "import datetime\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RecordWritingPath = '/Users/hemingyi/Documents/capstone/code/result/web/'\n",
    "TransportationDataPath = '/Users/hemingyi/Documents/capstone/transportation/'\n",
    "EventDataPath = '/Users/hemingyi/Documents/capstone/event data/update/'\n",
    "comboPath = '/Users/hemingyi/Documents/capstone/combo/'\n",
    "WebData = '/Users/hemingyi/Documents/capstone/webdata/'\n",
    "# dataFile = TransportationDataPath+city+'EdgeYearwiseAggregated.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readTransDF(TransportationDataPath,city):\n",
    "    df = pd.read_csv(TransportationDataPath+city+'EdgeYearwiseAggregated.csv')\n",
    "    return df\n",
    "def makeGraphfromDf(df):\n",
    "    G=nx.DiGraph()\n",
    "    nx.set_edge_attributes(G,'weight', 0)\n",
    "    for k in df.index:\n",
    "        G.add_edge(df['start_id'][k],df['end_id'][k],weight=df['amount'][k])\n",
    "#     nx.write_edgelist(G, comboPath+'temp/%s.net'%city)\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getComboPartition(G,comboPath,city):\n",
    "    #save network in net format\n",
    "    nodenum={}\n",
    "#     G = makeGraphfromDf(df)\n",
    "    #create a dictionary transforming nodes to unique numbers\n",
    "    nodes = list(G.nodes())\n",
    "#     print('nodes amount: ',len(nodes))\n",
    "    for i,j in enumerate(list(G.nodes())):\n",
    "        nodenum[str(j)] = str(i)\n",
    "#         nodes[str(i)] = str(j)\n",
    "#     i=0\n",
    "#     for n in list(G.nodes()):\n",
    "#         nodenum[n]=i\n",
    "#         nodes[i]=n\n",
    "#         i+=1\n",
    "\n",
    "    tempNetFile = comboPath+'temp/%s.net'%city\n",
    "    f = open(tempNetFile, 'w')\n",
    "    f.write('*Arcs\\n')\n",
    "\n",
    "    for e in G.edges(data=True):\n",
    "        f.write('{0} {1} {2}\\n'.format(nodenum[str(e[0])],nodenum[str(e[1])],e[2]['weight']))\n",
    "    f.close()\n",
    "\n",
    "    #run combo\n",
    "    command= comboPath+'/comboCPP '+tempNetFile#+' '+str(maxcom)\n",
    "    os.system(command)\n",
    "\n",
    "    #read resulting partition\n",
    "    partitionFile = comboPath+'temp/'+city + '_comm_comboC++.txt'\n",
    "    f = open(partitionFile, 'r')\n",
    "    i=0\n",
    "    partition={}\n",
    "    for line in f:\n",
    "        partition[str(nodes[i])]=str(int(line))\n",
    "        i+=1\n",
    "#         print(i)\n",
    "    f.close()\n",
    "    os.remove(partitionFile) \n",
    "    os.remove(tempNetFile)\n",
    "#     print(\"Communities: \",len(set(partition.values())))\n",
    "    return partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSubCommunity(df,maxComm=None):\n",
    "    data2 = df[df.start_community == df.end_community]\n",
    "    communities = list(set(df.start_community))\n",
    "#     print('communities numbers: ',len(communities))\n",
    "    SubPartition = {}\n",
    "    for c in communities:\n",
    "#         print('detecting sub-communities in community ',c)\n",
    "        d = data2[data2.start_community == c]\n",
    "        graph = makeGraphfromDf(d)\n",
    "        p = getComboPartition(graph,comboPath,city)\n",
    "        SubPartition[c] = p\n",
    "    df['start_community'] = df.apply(lambda x: str(x['start_community'])+'.'+str(SubPartition[x['start_community']][str(x['start_id'])]),axis=1)\n",
    "    df['end_community'] = df.apply(lambda x: str(x['end_community'])+'.'+str(SubPartition[x['end_community']][str(x['end_id'])]),axis=1)\n",
    "    communityNum = len(df['start_community'].unique())\n",
    "#     print('communityNum: ', communityNum)\n",
    "    if maxComm:\n",
    "        if communityNum >= maxComm:\n",
    "            return df\n",
    "        else:\n",
    "#             print('Continue commuity detection')\n",
    "            return(getSubCommunity(df))\n",
    "    else:\n",
    "        if communityNum >= 10:\n",
    "            return df\n",
    "        else:\n",
    "#             print('Continue commuity detection')\n",
    "            return(getSubCommunity(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anomalyDetection(y,ncom,pval = 0.2,iterN=20):\n",
    "    #index of regular (non-outlier points)\n",
    "\n",
    "    rind = np.array(range(y.shape[0]))\n",
    "    \n",
    "    #clustering model\n",
    "    gm=GaussianMixture(n_components=ncom,n_init=100,max_iter=1000,random_state=0) \n",
    "    for i in range(iterN): #iterate\n",
    "#         print('Iteration {}'.format(i+1))  \n",
    "        clustering=gm.fit(y[rind,:]) #fit EM clustering model excluding outliers\n",
    "        l=clustering.score_samples(y) #estimate likelihood for each point\n",
    "        Lthres=sorted(l)[int(len(l)*pval)] #anomaly threshold\n",
    "        rind0=0+rind\n",
    "        rind=l>Lthres #non-anomalous points\n",
    "        if all(rind==rind0):\n",
    "#             print('Convergence in {} iterations'.format(i+1))\n",
    "            break\n",
    "    return l < Lthres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import events data\n",
    "def getEvents(EventDataPath,city):\n",
    "    events_data =EventDataPath+city+'Events.csv'\n",
    "    df_events = pd.read_csv(events_data, encoding = \"ISO-8859-1\", parse_dates=['Date'], infer_datetime_format=True)\n",
    "\n",
    "    # dataframe for events\n",
    "    df_finalEvents =  df_events[['Date', 'Type']]\n",
    "\n",
    "    # list events666\n",
    "    \n",
    "    lis_event = df_finalEvents['Type'].unique()\n",
    "    lis_event = list(lis_event)\n",
    "    return (lis_event,df_finalEvents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregateByCommunities(commdata,city,TransportationDataPath):\n",
    "    commdata = pd.melt(commdata, id_vars=['start_id'], value_vars=['start_community']).drop_duplicates()[['start_id','value']]\n",
    "    data = pd.read_csv(TransportationDataPath+city+'EdgeDatewiseAggregated.csv')\n",
    "\n",
    "    print('Raw shape: ',data.shape)\n",
    "    if 'Date' in data.columns:\n",
    "        data['date'] = pd.to_datetime(data.Date)\n",
    "    elif 'date' in data.columns:\n",
    "        data['date'] = pd.to_datetime(data.date)\n",
    "    else:\n",
    "        print(data.columns)\n",
    "    data = data.merge(commdata, right_on='start_id', left_on='start_id')\n",
    "    data = data.merge(commdata, left_on='end_id',right_on='start_id' )\n",
    "    \n",
    "    communityData = data[['value_x','value_y','date','amount']]\n",
    "    communityData = communityData.groupby(['value_x','value_y','date']).sum().reset_index()\n",
    "    communityData.columns = ['start_id', 'end_id', 'date', 'amount']\n",
    "\n",
    "    return communityData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTimeSeries(df):\n",
    "    table = pd.pivot_table(df, values='amount', index=['date'],\n",
    "                    columns=['start_id','end_id'], aggfunc=np.sum, fill_value=0)\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anomalyDetection(y,ncomp,pval = 0.2,iterN=20):\n",
    "    #index of regular (non-outlier points)\n",
    "    #rind=y[:,0]>-10 \n",
    "    rind = np.array(range(y.shape[0]))\n",
    "    \n",
    "    #clustering model\n",
    "    gm=GaussianMixture(n_components=ncomp,n_init=100,max_iter=1000,random_state=0) \n",
    "    for i in range(iterN): #iterate\n",
    "#         print('Iteration {}'.format(i+1))  \n",
    "        clustering=gm.fit(y[rind,:]) #fit EM clustering model excluding outliers\n",
    "        l=clustering.score_samples(y) #estimate likelihood for each point\n",
    "        Lthres=sorted(l)[int(len(l)*pval)] #anomaly threshold\n",
    "        rind0=0+rind\n",
    "        rind=l>Lthres #non-anomalous points\n",
    "        if all(rind==rind0):\n",
    "#             print('Convergence in {} iterations'.format(i+1))\n",
    "            break\n",
    "    return l < Lthres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(city,maxComm=None):\n",
    "    f = open(RecordWritingPath+'F1ScoreSubCommunity.txt', 'a+')\n",
    "    print('Initialize')\n",
    "    df = readTransDF(TransportationDataPath,city)\n",
    "    lis_event,df_finalEvents = getEvents(EventDataPath,city)\n",
    "    print('Community Detection, aim at max', maxComm,' communities')\n",
    "    G = makeGraphfromDf(df)\n",
    "    partition = getComboPartition(G,comboPath,city)\n",
    "    df['start_community'] = df['start_id'].apply(lambda x: partition[str(x)])\n",
    "    df['end_community'] = df['end_id'].apply(lambda x: partition[str(x)])\n",
    "    Commdata = getSubCommunity(df)\n",
    "    print('aggregate date wise data by communities')\n",
    "    data = aggregateByCommunities(Commdata,city,TransportationDataPath)\n",
    "    print('Save Aggregated DF to csv')\n",
    "    data.to_csv(TransportationDataPath+'/CommDete/'+city+str(maxComm)+'Comm.csv',index=False)\n",
    "    dataTs = getTimeSeries(data)\n",
    "    matrix = dataTs.values\n",
    "    matrix = np.log(matrix+1)\n",
    "    \n",
    "    for i in range(matrix.shape[1]):\n",
    "        matrix[:, i] = (matrix[:, i] - matrix[:, i].min()) / (matrix[:, i].max() - matrix[:, i].min())\n",
    "    date = dataTs.index.to_frame().rename(columns={'date':'Date'})\n",
    "    threresult = {}\n",
    "    EventsDF = df_finalEvents['Date'].drop_duplicates().to_frame()\n",
    "    EventsDF['Anomaly'] = True\n",
    "    df = EventsDF.merge(date,on='Date',how='right')\n",
    "    df.fillna(False,inplace=True)\n",
    "    for comp in [1,2,3,4,5]:\n",
    "        print('n_component',comp)\n",
    "        for thres in range(1,10, 1):\n",
    "            th = thres/10\n",
    "    #         print(\"Threshhold: \",th)\n",
    "            outliers = anomalyDetection(matrix,comp,pval = th)\n",
    "            df['outlier'] = outliers\n",
    "\n",
    "            Precision = len(df[(df['outlier']==True)&(df['Anomaly']==True)])/len(df[df['outlier']==True])\n",
    "            Recall = len(df[(df['outlier']==True)&(df['Anomaly']==True)])/len(EventsDF)\n",
    "            if Precision+Recall > 0:\n",
    "                F1 = (2*Precision*Recall)/(Precision+Recall)\n",
    "            else:\n",
    "                F1 = 0\n",
    "            threresult[th] = F1\n",
    "        bestTh = max(threresult.items(), key=operator.itemgetter(1))[0]\n",
    "        print('best F1 score in',comp,' components is ',str(threresult[bestTh]))\n",
    "        #use best th to calculate again\n",
    "\n",
    "        outliers = anomalyDetection(matrix,comp,pval = bestTh)\n",
    "        df['outlier'] = outliers\n",
    "        TP = len(df[(df['outlier']==True)&(df['Anomaly']==True)])\n",
    "        FP = len(df[(df['outlier']==False)&(df['Anomaly']==True)])\n",
    "        TN = len(df[(df['outlier']==False)&(df['Anomaly']==False)])\n",
    "        FN = len(df[(df['outlier']==False)&(df['Anomaly']==True)])\n",
    "        if FP + TN > 0:\n",
    "            FPR = FP / (FP + TN)\n",
    "        else:\n",
    "            FPR = 0\n",
    "\n",
    "        f.write(city+',Comm + GMM,'+str(comp)+','+str(threresult[bestTh])+','+str(TP)+','+str(FP)+','+str(TN)+','+str(FN)+',')\n",
    "        for event in ['Global Event','Local Event']:\n",
    "#             print(event)\n",
    "            SingleEventDF = df_finalEvents[df_finalEvents['Type'] == event]\n",
    "            SingleEventDF = SingleEventDF.drop_duplicates()\n",
    "            SingleDF = date.merge(SingleEventDF, on='Date', how='left')\n",
    "        #     SingleDF = SingleEventDF.merge(date,on='Date',how='left')\n",
    "            SingleDF['outliers'] = outliers\n",
    "        #     SinglePrecision = len(SingleDF[(SingleDF['outlier']==True)&(SingleDF['Type'].notnull)])/len(SingleDF[SingleDF['outlier']==True])\n",
    "            SingleRecall = len(SingleDF[(SingleDF['outliers']==True)&(SingleDF['Type'].notnull())])/len(SingleEventDF)\n",
    "            SingleFPR = len(SingleDF[(SingleDF['outliers']==True)&(SingleDF['Type'].isna())])/len(SingleDF[SingleDF['Type'].isna()])\n",
    "            f.write(str(SingleRecall)+',')\n",
    "        f.write('\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(city,maxComm=None):\n",
    "    f = open(RecordWritingPath+'0702.csv', 'a+')\n",
    "#     print('Initialize')\n",
    "#     df = readTransDF(TransportationDataPath,city)\n",
    "    lis_event,df_finalEvents = getEvents(EventDataPath,city)\n",
    "#     print('Community Detection, aim at max', maxComm,' communities')\n",
    "#     G = makeGraphfromDf(df)\n",
    "#     partition = getComboPartition(G,comboPath,city)\n",
    "#     df['start_community'] = df['start_id'].apply(lambda x: partition[str(x)])\n",
    "#     df['end_community'] = df['end_id'].apply(lambda x: partition[str(x)])\n",
    "#     Commdata = getSubCommunity(df)\n",
    "#     print('aggregate date wise data by communities')\n",
    "#     data = aggregateByCommunities(Commdata,city,TransportationDataPath)\n",
    "#     print('Save Aggregated DF to csv')\n",
    "    data = pd.read_csv(TransportationDataPath+'/CommDete/'+city+str(maxComm)+'Comm.csv')\n",
    "#     data.to_csv(TransportationDataPath+'/CommDete/'+city+str(maxComm)+'Comm.csv',index=False)\n",
    "    dataTs = getTimeSeries(data)\n",
    "    matrix = dataTs.values\n",
    "    matrix = np.log(matrix+1)\n",
    "    \n",
    "    for i in range(matrix.shape[1]):\n",
    "        matrix[:, i] = (matrix[:, i] - matrix[:, i].min()) / (matrix[:, i].max() - matrix[:, i].min())\n",
    "    date = dataTs.index.to_frame().rename(columns={'date':'Date'})\n",
    "    threresult = {}\n",
    "    EventsDF = df_finalEvents['Date'].drop_duplicates().to_frame()\n",
    "    EventsDF['Anomaly'] = True\n",
    "    EventsDF['Date'] = EventsDF['Date'].astype('str')\n",
    "    date['Date'] = date['Date'].astype('str')\n",
    "    df = EventsDF.merge(date,on='Date',how='right')\n",
    "    df.fillna(False,inplace=True)\n",
    "    for comp in [1,2,3,4,5]:\n",
    "        print('n_component',comp)\n",
    "        for thres in range(1,10, 1):\n",
    "            th = thres/10\n",
    "    #         print(\"Threshhold: \",th)\n",
    "            outliers = anomalyDetection(matrix,comp,pval = th)\n",
    "            df['outlier'] = outliers\n",
    "            TP = len(df[(df['outlier']==True)&(df['Anomaly']==True)])\n",
    "            FP = len(df[(df['outlier']==False)&(df['Anomaly']==True)])\n",
    "            TN = len(df[(df['outlier']==False)&(df['Anomaly']==False)])\n",
    "            FN = len(df[(df['outlier']==False)&(df['Anomaly']==True)])\n",
    "            Precision = TP/(TP+FP)\n",
    "            Recall = TP/(FP+FN)\n",
    "            if Precision+Recall > 0:\n",
    "                F1 = (2*Precision*Recall)/(Precision+Recall)\n",
    "            else:\n",
    "                F1 = 0\n",
    "            threresult[th] = F1\n",
    "        bestTh = max(threresult.items(), key=operator.itemgetter(1))[0]\n",
    "        print('best F1 score in',comp,' components is ',str(threresult[bestTh]))\n",
    "        #use best th to calculate again\n",
    "\n",
    "        outliers = anomalyDetection(matrix,comp,pval = bestTh)\n",
    "        df['outlier'] = outliers\n",
    "        TP = len(df[(df['outlier']==True)&(df['Anomaly']==True)])\n",
    "        FP = len(df[(df['outlier']==False)&(df['Anomaly']==True)])\n",
    "        TN = len(df[(df['outlier']==False)&(df['Anomaly']==False)])\n",
    "        FN = len(df[(df['outlier']==False)&(df['Anomaly']==True)])\n",
    "\n",
    "        FPR = FP / (FP + TN)\n",
    "\n",
    "        f.write(city+',Comm + GMM,'+str(comp)+','+str(threresult[bestTh])+','+str(TP)+','+str(FP)+','+str(TN)+','+str(FN)+',')\n",
    "        for event in ['National Holiday', 'Culture Event', 'Extreme Weather']:\n",
    "#             print(event)\n",
    "            SingleEventDF = df_finalEvents[df_finalEvents['Type'] == event]\n",
    "            SingleEventDF = SingleEventDF.drop_duplicates()\n",
    "            SingleEventDF['Date'] = SingleEventDF['Date'].astype('str')\n",
    "            SingleDF = date.merge(SingleEventDF, on='Date', how='left')\n",
    "            \n",
    "        #     SingleDF = SingleEventDF.merge(date,on='Date',how='left')\n",
    "            SingleDF['outliers'] = outliers\n",
    "        #     SinglePrecision = len(SingleDF[(SingleDF['outlier']==True)&(SingleDF['Type'].notnull)])/len(SingleDF[SingleDF['outlier']==True])\n",
    "            SingleRecall = len(SingleDF[(SingleDF['outliers']==True)&(SingleDF['Type'].notnull())])/len(SingleEventDF)\n",
    "            SingleFPR = len(SingleDF[(SingleDF['outliers']==True)&(SingleDF['Type'].isna())])/len(SingleDF[SingleDF['Type'].isna()])\n",
    "            f.write(str(SingleRecall)+',')\n",
    "        f.write('\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(RecordWritingPath+'0702.csv', 'w+')\n",
    "f.write('city,Method,GMM Component,GMM Threshold,TP,FP,TN,FN,National Holiday,Culture Event,Extreme Weather\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(city,maxComm=None):\n",
    "    f = open(RecordWritingPath+'0702.csv', 'a+')\n",
    "#     print('Initialize')\n",
    "#     df = readTransDF(TransportationDataPath,city)\n",
    "    lis_event,df_finalEvents = getEvents(EventDataPath,city)\n",
    "#     print('Community Detection, aim at max', maxComm,' communities')\n",
    "#     G = makeGraphfromDf(df)\n",
    "#     partition = getComboPartition(G,comboPath,city)\n",
    "#     df['start_community'] = df['start_id'].apply(lambda x: partition[str(x)])\n",
    "#     df['end_community'] = df['end_id'].apply(lambda x: partition[str(x)])\n",
    "#     Commdata = getSubCommunity(df)\n",
    "#     print('aggregate date wise data by communities')\n",
    "#     data = aggregateByCommunities(Commdata,city,TransportationDataPath)\n",
    "#     print('Save Aggregated DF to csv')\n",
    "    data = pd.read_csv(TransportationDataPath+'/CommDete/'+city+str(maxComm)+'Comm.csv')\n",
    "#     data.to_csv(TransportationDataPath+'/CommDete/'+city+str(maxComm)+'Comm.csv',index=False)\n",
    "    dataTs = getTimeSeries(data)\n",
    "    matrix = dataTs.values\n",
    "    matrix = np.log(matrix+1)\n",
    "    \n",
    "    for i in range(matrix.shape[1]):\n",
    "        matrix[:, i] = (matrix[:, i] - matrix[:, i].min()) / (matrix[:, i].max() - matrix[:, i].min())\n",
    "    date = dataTs.index.to_frame().rename(columns={'date':'Date'})\n",
    "    threresult = {}\n",
    "    EventsDF = df_finalEvents['Date'].drop_duplicates().to_frame()\n",
    "    EventsDF['Anomaly'] = True\n",
    "    EventsDF['Date'] = EventsDF['Date'].astype('str')\n",
    "    date['Date'] = date['Date'].astype('str')\n",
    "    df = EventsDF.merge(date,on='Date',how='right')\n",
    "    df.fillna(False,inplace=True)\n",
    "    for comp in [1,2,3,4,5]:\n",
    "        print('n_component',comp)\n",
    "        for thres in range(1,10, 1):\n",
    "            th = thres/10\n",
    "    #         print(\"Threshhold: \",th)\n",
    "            outliers = anomalyDetection(matrix,comp,pval = th)\n",
    "            df['outlier'] = outliers\n",
    "            TP = len(df[(df['outlier']==True)&(df['Anomaly']==True)])\n",
    "            FP = len(df[(df['outlier']==False)&(df['Anomaly']==True)])\n",
    "            TN = len(df[(df['outlier']==False)&(df['Anomaly']==False)])\n",
    "            FN = len(df[(df['outlier']==True)&(df['Anomaly']==False)])\n",
    "            f.write(city+',Comm + GMM,'+str(comp)+','+str(th)+','+str(TP)+','+str(FP)+','+str(TN)+','+str(FN)+',')\n",
    "#             print(city+',Comm + GMM,'+str(comp)+','+str(th)+','+str(TP)+','+str(FP)+','+str(TN)+','+str(FN)+',')\n",
    "            for event in ['National Holiday', 'Culture Event', 'Extreme Weather']:\n",
    "    #             print(event)\n",
    "                SingleEventDF = df_finalEvents[df_finalEvents['Type'] == event]\n",
    "                SingleEventDF = SingleEventDF.drop_duplicates()\n",
    "                SingleEventDF['Date'] = SingleEventDF['Date'].astype('str')\n",
    "                SingleDF = date.merge(SingleEventDF, on='Date', how='left')\n",
    "\n",
    "            #     SingleDF = SingleEventDF.merge(date,on='Date',how='left')\n",
    "                SingleDF['outliers'] = outliers\n",
    "            #     SinglePrecision = len(SingleDF[(SingleDF['outlier']==True)&(SingleDF['Type'].notnull)])/len(SingleDF[SingleDF['outlier']==True])\n",
    "                SingleRecall = len(SingleDF[(SingleDF['outliers']==True)&(SingleDF['Type'].notnull())])/len(SingleEventDF)\n",
    "                SingleFPR = len(SingleDF[(SingleDF['outliers']==True)&(SingleDF['Type'].isna())])/len(SingleDF[SingleDF['Type'].isna()])\n",
    "                f.write(str(SingleRecall)+',')\n",
    "            f.write('\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_component 1\n",
      "n_component 2\n",
      "n_component 3\n",
      "n_component 4\n",
      "n_component 5\n"
     ]
    }
   ],
   "source": [
    "pipeline('DC',9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_component 1\n",
      "n_component 2\n",
      "n_component 3\n",
      "n_component 4\n",
      "n_component 5\n"
     ]
    }
   ],
   "source": [
    "pipeline('Taipei',10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_component 1\n",
      "n_component 2\n",
      "n_component 3\n",
      "n_component 4\n",
      "n_component 5\n"
     ]
    }
   ],
   "source": [
    "pipeline('Chicago',9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_component 1\n",
      "n_component 2\n",
      "n_component 3\n",
      "n_component 4\n",
      "n_component 5\n"
     ]
    }
   ],
   "source": [
    "pipeline('NewYork',6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.1 725 725 9 73 580 63 725\n",
      "1 0.2 725 725 17 65 515 128 725\n",
      "1 0.3 725 725 24 58 450 193 725\n",
      "1 0.4 725 725 30 52 383 260 725\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-155-0e63236ff633>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthres\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0moutliers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manomalyDetection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcomp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'outlier'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutliers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mTP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'outlier'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m&\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Anomaly'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-58-94bb5a6f2031>\u001b[0m in \u001b[0;36manomalyDetection\u001b[0;34m(y, ncomp, pval, iterN)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#iterate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#         print('Iteration {}'.format(i+1))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mclustering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrind\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#fit EM clustering model excluding outliers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0ml\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclustering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#estimate likelihood for each point\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mLthres\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#anomaly threshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/mixture/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \"\"\"\n\u001b[0;32m--> 194\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/mixture/base.py\u001b[0m in \u001b[0;36mfit_predict\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    242\u001b[0m                 \u001b[0mprev_lower_bound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlower_bound\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m                 \u001b[0mlog_prob_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_resp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_e_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_resp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m                 lower_bound = self._compute_lower_bound(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/mixture/base.py\u001b[0m in \u001b[0;36m_e_step\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0mthe\u001b[0m \u001b[0mpoint\u001b[0m \u001b[0mof\u001b[0m \u001b[0meach\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m         \"\"\"\n\u001b[0;32m--> 297\u001b[0;31m         \u001b[0mlog_prob_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_resp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_estimate_log_prob_resp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_prob_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_resp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/mixture/base.py\u001b[0m in \u001b[0;36m_estimate_log_prob_resp\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    504\u001b[0m             \u001b[0mlogarithm\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mresponsibilities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m         \"\"\"\n\u001b[0;32m--> 506\u001b[0;31m         \u001b[0mweighted_log_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_estimate_weighted_log_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    507\u001b[0m         \u001b[0mlog_prob_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogsumexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweighted_log_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/mixture/base.py\u001b[0m in \u001b[0;36m_estimate_weighted_log_prob\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0mweighted_log_prob\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_component\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m         \"\"\"\n\u001b[0;32m--> 459\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_estimate_log_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_estimate_log_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/mixture/gaussian_mixture.py\u001b[0m in \u001b[0;36m_estimate_log_prob\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_estimate_log_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m         return _estimate_log_gaussian_prob(\n\u001b[0;32m--> 682\u001b[0;31m             X, self.means_, self.precisions_cholesky_, self.covariance_type)\n\u001b[0m\u001b[1;32m    683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_estimate_log_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/mixture/gaussian_mixture.py\u001b[0m in \u001b[0;36m_estimate_log_gaussian_prob\u001b[0;34m(X, means, precisions_chol, covariance_type)\u001b[0m\n\u001b[1;32m    410\u001b[0m         \u001b[0mlog_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprec_chol\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecisions_chol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprec_chol\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprec_chol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m             \u001b[0mlog_prob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# def pipeline(city,maxComm=None):\n",
    "# f = open(RecordWritingPath+'0702.csv', 'a+')\n",
    "#     print('Initialize')\n",
    "#     df = readTransDF(TransportationDataPath,city)\n",
    "city,maxComm = 'DC',9\n",
    "lis_event,df_finalEvents = getEvents(EventDataPath,city)\n",
    "\n",
    "data = pd.read_csv(TransportationDataPath+'/CommDete/'+city+str(maxComm)+'Comm.csv')\n",
    "dataTs = getTimeSeries(data)\n",
    "matrix = dataTs.values\n",
    "matrix = np.log(matrix+1)\n",
    "\n",
    "for i in range(matrix.shape[1]):\n",
    "    matrix[:, i] = (matrix[:, i] - matrix[:, i].min()) / (matrix[:, i].max() - matrix[:, i].min())\n",
    "date = dataTs.index.to_frame().rename(columns={'date':'Date'})\n",
    "threresult = {}\n",
    "EventsDF = df_finalEvents['Date'].drop_duplicates().to_frame()\n",
    "EventsDF['Anomaly'] = True\n",
    "EventsDF['Date'] = EventsDF['Date'].astype('str')\n",
    "date['Date'] = date['Date'].astype('str')\n",
    "df = EventsDF.merge(date,on='Date',how='right')\n",
    "df.fillna(False,inplace=True)\n",
    "for comp in [1,2,3,4,5]:\n",
    "       \n",
    "    for thres in range(1,10, 1):\n",
    "        th = thres/10\n",
    "        \n",
    "        outliers = anomalyDetection(matrix,comp,pval = th)\n",
    "        df['outlier'] = outliers\n",
    "        TP = len(df[(df['outlier']==True)&(df['Anomaly']==True)])\n",
    "        FP = len(df[(df['outlier']==False)&(df['Anomaly']==True)])\n",
    "        TN = len(df[(df['outlier']==False)&(df['Anomaly']==False)])\n",
    "        FN = len(df[(df['outlier']==True)&(df['Anomaly']==False)])\n",
    "        print(comp, th, len(df), len(outliers),TP, FP, TN, FN, (TP+FP+TN+FN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True, False])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.outlier.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True, False])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Anomaly.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
