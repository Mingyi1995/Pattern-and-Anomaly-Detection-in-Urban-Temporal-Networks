{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.mixture import GaussianMixture \n",
    "from sklearn.decomposition import PCA\n",
    "import datetime\n",
    "import operator\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "RecordWritingPath = '/Users/hemingyi/Documents/capstone/code/result/web/'\n",
    "TransportationDataPath = '/Users/hemingyi/Documents/capstone/transportation/'\n",
    "EventDataPath = '/Users/hemingyi/Documents/capstone/event data/update/'\n",
    "comboPath = '/Users/hemingyi/Documents/capstone/combo/'\n",
    "WebData = '/Users/hemingyi/Documents/capstone/webdata/'\n",
    "# dataFile = TransportationDataPath+city+'EdgeYearwiseAggregated.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taipei\n",
      "done\n",
      "DC\n",
      "done\n",
      "Chicago\n",
      "done\n",
      "NewYork\n",
      "done\n",
      "all done\n"
     ]
    }
   ],
   "source": [
    "for city in ['Taipei','DC','Chicago','NewYork']:\n",
    "    print(city)\n",
    "    data = pd.read_csv(TransportationDataPath+city+'EdgeDatewiseAggregated.csv')\n",
    "    startId = data.groupby(['start_id','date']).sum()\n",
    "    endId = data.groupby(['end_id','date']).sum()\n",
    "    startPivot = pd.pivot_table(startId, values='amount', index=['date'],\n",
    "                        columns=['start_id'], aggfunc=sum, fill_value=0)\n",
    "    # startPivot.columns = [x+'start' for x in startPivot.columns]\n",
    "    endPivot = pd.pivot_table(endId, values='amount', index=['date'],\n",
    "                        columns=['end_id'], aggfunc=sum, fill_value=0)\n",
    "    # endPivot.columns = [x+'end' for x in endPivot.columns]\n",
    "    startPivot.to_csv(WebData+city+'inflow.csv')\n",
    "    endPivot.to_csv(WebData+city+'outflow.csv')\n",
    "    print('done')\n",
    "print('all done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taipei\n",
      "done\n",
      "DC\n",
      "done\n",
      "Chicago\n",
      "done\n",
      "NewYork\n",
      "done\n",
      "all done\n"
     ]
    }
   ],
   "source": [
    "for city in ['Taipei','DC','Chicago','NewYork']:\n",
    "    print(city)\n",
    "    data = pd.read_csv(TransportationDataPath+city+'EdgeDatewiseAggregated.csv')\n",
    "    startId = data.groupby(['start_id','date']).sum()\n",
    "    endId = data.groupby(['end_id','date']).sum()\n",
    "    startPivot = pd.pivot_table(startId, values='amount', index=['date'],\n",
    "                        columns=['start_id'], aggfunc=sum, fill_value=0)\n",
    "    startPivot.columns = [str(x)+'start' for x in startPivot.columns]\n",
    "    endPivot = pd.pivot_table(endId, values='amount', index=['date'],\n",
    "                        columns=['end_id'], aggfunc=sum, fill_value=0)\n",
    "    endPivot.columns = [str(x)+'end' for x in endPivot.columns]\n",
    "    InOutFlow = startPivot.merge(endPivot, on='date')\n",
    "    InOutFlow.to_csv(TransportationDataPath+city+'InOutFlow.csv',index=True)\n",
    "    print('done')\n",
    "print('all done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "startPivot = pd.pivot_table(startId, values='amount', index=['date'],\n",
    "                    columns=['start_id'], aggfunc=sum, fill_value=0)\n",
    "# startPivot.columns = [x+'start' for x in startPivot.columns]\n",
    "endPivot = pd.pivot_table(endId, values='amount', index=['date'],\n",
    "                    columns=['end_id'], aggfunc=sum, fill_value=0)\n",
    "# endPivot.columns = [x+'end' for x in endPivot.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anomalyDetection(y,ncom,pval = 0.2,iterN=20):\n",
    "    #index of regular (non-outlier points)\n",
    "\n",
    "    rind = np.array(range(y.shape[0]))\n",
    "    \n",
    "    #clustering model\n",
    "    gm=GaussianMixture(n_components=ncom,n_init=100,max_iter=1000,random_state=0) \n",
    "    for i in range(iterN): #iterate\n",
    "#         print('Iteration {}'.format(i+1))  \n",
    "        clustering=gm.fit(y[rind,:]) #fit EM clustering model excluding outliers\n",
    "        l=clustering.score_samples(y) #estimate likelihood for each point\n",
    "        Lthres=sorted(l)[int(len(l)*pval)] #anomaly threshold\n",
    "#         print(Lthres)\n",
    "        rind0=0+rind\n",
    "        rind=l>Lthres #non-anomalous points\n",
    "        if all(rind==rind0):\n",
    "#             print('Convergence in {} iterations'.format(i+1))\n",
    "            break\n",
    "    return l < Lthres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import events data\n",
    "def getEvents(EventDataPath,city):\n",
    "    events_data =EventDataPath+city+'Events.csv'\n",
    "    df_events = pd.read_csv(events_data, encoding = \"ISO-8859-1\", parse_dates=['Date'], infer_datetime_format=True)\n",
    "\n",
    "    # dataframe for events\n",
    "    df_finalEvents =  df_events[['Date', 'Type']]\n",
    "\n",
    "    # list events666\n",
    "    \n",
    "    lis_event = df_finalEvents['Type'].unique()\n",
    "    lis_event = list(lis_event)\n",
    "    return (lis_event,df_finalEvents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTimeSeries(df):\n",
    "    table = pd.pivot_table(df, values='amount', index=['date'],\n",
    "                    columns=['start_id','end_id'], aggfunc=np.sum, fill_value=0)\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anomalyDetection(y,ncomp,pval = 0.2,iterN=20):\n",
    "    #index of regular (non-outlier points)\n",
    "    #rind=y[:,0]>-10 \n",
    "    rind = np.array(range(y.shape[0]))\n",
    "    \n",
    "    #clustering model\n",
    "    gm=GaussianMixture(n_components=ncomp,n_init=100,max_iter=1000,random_state=0) \n",
    "    for i in range(iterN): #iterate\n",
    "#         print('Iteration {}'.format(i+1))  \n",
    "        clustering=gm.fit(y[rind,:]) #fit EM clustering model excluding outliers\n",
    "        l=clustering.score_samples(y) #estimate likelihood for each point\n",
    "        Lthres=sorted(l)[int(len(l)*pval)] #anomaly threshold\n",
    "        rind0=0+rind\n",
    "        rind=l>Lthres #non-anomalous points\n",
    "        if all(rind==rind0):\n",
    "#             print('Convergence in {} iterations'.format(i+1))\n",
    "            break\n",
    "    return l < Lthres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(city):\n",
    "    f = open(RecordWritingPath+'InOutFlowPCA.csv', 'a+')\n",
    "    print('Initialize')\n",
    "    lis_event,df_finalEvents = getEvents(EventDataPath,city)\n",
    "    data = pd.read_csv(TransportationDataPath+city+'InOutFlow.csv')\n",
    "    \n",
    "#     dataTs = getTimeSeries(data)\n",
    "#     dataTs = dataTs.sort_index()\n",
    "    matrix = data.drop(['date'], axis=1).values\n",
    "    matrix = np.log(matrix+1)\n",
    "    for i in range(matrix.shape[1]):\n",
    "        matrix[:, i] = (matrix[:, i] - matrix[:, i].min()) / (matrix[:, i].max() - matrix[:, i].min())\n",
    "    pca = PCA(n_components=16)\n",
    "    matrix=pca.fit_transform(matrix)\n",
    "    columns = []\n",
    "    for i in range(1,17):\n",
    "        columns += ['Component%s'%i]\n",
    "    outputMatrix = pd.DataFrame(data = matrix, index=date.Date.tolist(), columns=columns)\n",
    "    outputMatrix.to_csv(WebData+city+'AutoEncoder.csv')\n",
    "    date = data.date.to_frame().rename(columns={'date':'Date'})\n",
    "#     print(date.head())\n",
    "    threresult = {}\n",
    "    EventsDF = df_finalEvents.drop_duplicates(subset='Date', keep='first', inplace=False)\n",
    "    EventsDF['Anomaly'] = True\n",
    "    EventsDF['Date'] = EventsDF['Date'].astype('str')\n",
    "    date['Date'] = date['Date'].astype('str')\n",
    "    df = date.merge(EventsDF,on='Date',how='left')\n",
    "    df.Anomaly.fillna(False, inplace=True)\n",
    "    for comp in [1,2,3,4,5]:\n",
    "        print('n_component',comp)\n",
    "        for thres in list(range(1,10,1))+[10*len(df[df['Anomaly']==True])/len(df)]:\n",
    "            th = thres/10\n",
    "            outliers = anomalyDetection(matrix,comp,pval = th)\n",
    "            df['outlier'] = outliers\n",
    "            TP = len(df[(df['outlier']==True)&(df['Anomaly']==True)])\n",
    "            FP = len(df[(df['outlier']==True)&(df['Anomaly']==False)])\n",
    "            TN = len(df[(df['outlier']==False)&(df['Anomaly']==False)])\n",
    "            FN = len(df[(df['outlier']==False)&(df['Anomaly']==True)])\n",
    "            f.write(city+',Comm + PCA + GMM,'+str(comp)+','+str(th)+','+str(TP)+','+str(FP)+','+str(TN)+','+str(FN)+',')\n",
    "            for event in ['National Holiday', 'Culture Event', 'Extreme Weather']:\n",
    "    #             print(event)\n",
    "                T = len(df[df['Type']==event])\n",
    "                TP = len(df[(df['Type']==event)&(df['outlier']==True)])\n",
    "                TPR = TP/T\n",
    "                f.write(str(TPR)+',')\n",
    "            f.write('\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NewYork\n",
      "Initialize\n",
      "saved pca matrix\n",
      "n_component 1\n",
      "n_component 2\n",
      "n_component 3\n",
      "n_component 4\n",
      "n_component 5\n"
     ]
    }
   ],
   "source": [
    "for city in ['NewYork']:\n",
    "    print(city)\n",
    "    pipeline(city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize\n"
     ]
    }
   ],
   "source": [
    "city = 'Chicago'\n",
    "print('Initialize')\n",
    "data = pd.read_csv(TransportationDataPath+city+'InOutFlow.csv')\n",
    "# del data['Unnamed: 0']\n",
    "date = data['date']\n",
    "matrix = data.drop(['date'], axis=1).values\n",
    "matrix = np.log(matrix+1)\n",
    "for i in range(matrix.shape[1]):\n",
    "    matrix[:, i] = (matrix[:, i] - matrix[:, i].min()) / (matrix[:, i].max() - matrix[:, i].min())\n",
    "pca = PCA(n_components=16)\n",
    "matrix=pca.fit_transform(matrix)\n",
    "columns = []\n",
    "for i in range(1,17):\n",
    "    columns += ['Component%s'%i]\n",
    "outputMatrix = pd.DataFrame(data = matrix, index=date.tolist(), columns=columns)\n",
    "outputMatrix.to_csv(WebData+'In&OutAutoEncoder/'+city+'PCA.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
