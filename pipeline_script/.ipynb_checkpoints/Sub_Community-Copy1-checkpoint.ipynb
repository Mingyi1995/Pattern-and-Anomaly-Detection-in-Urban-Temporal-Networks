{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add necessary libraries\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.mixture import GaussianMixture \n",
    "import datetime\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RecordWritingPath = '/Users/hemingyi/Documents/capstone/code/result/web/'\n",
    "TransportationDataPath = '/Users/hemingyi/Documents/capstone/transportation/'\n",
    "EventDataPath = '/Users/hemingyi/Documents/capstone/event data/update/'\n",
    "comboPath = '/Users/hemingyi/Documents/capstone/combo/'\n",
    "WebData = '/Users/hemingyi/Documents/capstone/webdata/'\n",
    "# dataFile = TransportationDataPath+city+'EdgeYearwiseAggregated.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readTransDF(TransportationDataPath,city):\n",
    "    df = pd.read_csv(TransportationDataPath+city+'EdgeYearwiseAggregated.csv')\n",
    "    return df\n",
    "def makeGraphfromDf(df):\n",
    "    G=nx.DiGraph()\n",
    "    nx.set_edge_attributes(G,'weight', 0)\n",
    "    for k in df.index:\n",
    "        G.add_edge(df['start_id'][k],df['end_id'][k],weight=df['amount'][k])\n",
    "#     nx.write_edgelist(G, comboPath+'temp/%s.net'%city)\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getComboPartition(G,comboPath,city):\n",
    "    #save network in net format\n",
    "    nodenum={}\n",
    "#     G = makeGraphfromDf(df)\n",
    "    #create a dictionary transforming nodes to unique numbers\n",
    "    nodes = list(G.nodes())\n",
    "#     print('nodes amount: ',len(nodes))\n",
    "    for i,j in enumerate(list(G.nodes())):\n",
    "        nodenum[str(j)] = str(i)\n",
    "#         nodes[str(i)] = str(j)\n",
    "#     i=0\n",
    "#     for n in list(G.nodes()):\n",
    "#         nodenum[n]=i\n",
    "#         nodes[i]=n\n",
    "#         i+=1\n",
    "\n",
    "    tempNetFile = comboPath+'temp/%s.net'%city\n",
    "    f = open(tempNetFile, 'w')\n",
    "    f.write('*Arcs\\n')\n",
    "\n",
    "    for e in G.edges(data=True):\n",
    "        f.write('{0} {1} {2}\\n'.format(nodenum[str(e[0])],nodenum[str(e[1])],e[2]['weight']))\n",
    "    f.close()\n",
    "\n",
    "    #run combo\n",
    "    command= comboPath+'/comboCPP '+tempNetFile#+' '+str(maxcom)\n",
    "    os.system(command)\n",
    "\n",
    "    #read resulting partition\n",
    "    partitionFile = comboPath+'temp/'+city + '_comm_comboC++.txt'\n",
    "    f = open(partitionFile, 'r')\n",
    "    i=0\n",
    "    partition={}\n",
    "    for line in f:\n",
    "        partition[str(nodes[i])]=str(int(line))\n",
    "        i+=1\n",
    "#         print(i)\n",
    "    f.close()\n",
    "    os.remove(partitionFile) \n",
    "    os.remove(tempNetFile)\n",
    "#     print(\"Communities: \",len(set(partition.values())))\n",
    "    return partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSubCommunity(df,maxComm=None):\n",
    "    data2 = df[df.start_community == df.end_community]\n",
    "    communities = list(set(df.start_community))\n",
    "#     print('communities numbers: ',len(communities))\n",
    "    SubPartition = {}\n",
    "    for c in communities:\n",
    "#         print('detecting sub-communities in community ',c)\n",
    "        d = data2[data2.start_community == c]\n",
    "        graph = makeGraphfromDf(d)\n",
    "        p = getComboPartition(graph,comboPath,city)\n",
    "        SubPartition[c] = p\n",
    "    df['start_community'] = df.apply(lambda x: str(x['start_community'])+'.'+str(SubPartition[x['start_community']][str(x['start_id'])]),axis=1)\n",
    "    df['end_community'] = df.apply(lambda x: str(x['end_community'])+'.'+str(SubPartition[x['end_community']][str(x['end_id'])]),axis=1)\n",
    "    communityNum = len(df['start_community'].unique())\n",
    "#     print('communityNum: ', communityNum)\n",
    "    if maxComm:\n",
    "        if communityNum >= maxComm:\n",
    "            return df\n",
    "        else:\n",
    "#             print('Continue commuity detection')\n",
    "            return(getSubCommunity(df))\n",
    "    else:\n",
    "        if communityNum >= 10:\n",
    "            return df\n",
    "        else:\n",
    "#             print('Continue commuity detection')\n",
    "            return(getSubCommunity(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anomalyDetection(y,ncom,pval = 0.2,iterN=20):\n",
    "    #index of regular (non-outlier points)\n",
    "\n",
    "    rind = np.array(range(y.shape[0]))\n",
    "    \n",
    "    #clustering model\n",
    "    gm=GaussianMixture(n_components=ncom,n_init=100,max_iter=1000,random_state=0) \n",
    "    for i in range(iterN): #iterate\n",
    "#         print('Iteration {}'.format(i+1))  \n",
    "        clustering=gm.fit(y[rind,:]) #fit EM clustering model excluding outliers\n",
    "        l=clustering.score_samples(y) #estimate likelihood for each point\n",
    "        Lthres=sorted(l)[int(len(l)*pval)] #anomaly threshold\n",
    "        rind0=0+rind\n",
    "        rind=l>Lthres #non-anomalous points\n",
    "        if all(rind==rind0):\n",
    "#             print('Convergence in {} iterations'.format(i+1))\n",
    "            break\n",
    "    return l < Lthres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import events data\n",
    "def getEvents(EventDataPath,city):\n",
    "    events_data =EventDataPath+city+'Events.csv'\n",
    "    df_events = pd.read_csv(events_data, encoding = \"ISO-8859-1\", parse_dates=['Date'], infer_datetime_format=True)\n",
    "\n",
    "    # dataframe for events\n",
    "    df_finalEvents =  df_events[['Date', 'Type']]\n",
    "\n",
    "    # list events666\n",
    "    \n",
    "    lis_event = df_finalEvents['Type'].unique()\n",
    "    lis_event = list(lis_event)\n",
    "    return (lis_event,df_finalEvents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregateByCommunities(commdata,city,TransportationDataPath):\n",
    "    commdata = pd.melt(commdata, id_vars=['start_id'], value_vars=['start_community']).drop_duplicates()[['start_id','value']]\n",
    "    data = pd.read_csv(TransportationDataPath+city+'EdgeDatewiseAggregated.csv')\n",
    "\n",
    "    print('Raw shape: ',data.shape)\n",
    "    if 'Date' in data.columns:\n",
    "        data['date'] = pd.to_datetime(data.Date)\n",
    "    elif 'date' in data.columns:\n",
    "        data['date'] = pd.to_datetime(data.date)\n",
    "    else:\n",
    "        print(data.columns)\n",
    "    data = data.merge(commdata, right_on='start_id', left_on='start_id')\n",
    "    data = data.merge(commdata, left_on='end_id',right_on='start_id' )\n",
    "    \n",
    "    communityData = data[['value_x','value_y','date','amount']]\n",
    "    communityData = communityData.groupby(['value_x','value_y','date']).sum().reset_index()\n",
    "    communityData.columns = ['start_id', 'end_id', 'date', 'amount']\n",
    "\n",
    "    return communityData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTimeSeries(df):\n",
    "    table = pd.pivot_table(df, values='amount', index=['date'],\n",
    "                    columns=['start_id','end_id'], aggfunc=np.sum, fill_value=0)\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anomalyDetection(y,ncomp,pval = 0.2,iterN=20):\n",
    "    #index of regular (non-outlier points)\n",
    "    #rind=y[:,0]>-10 \n",
    "    rind = np.array(range(y.shape[0]))\n",
    "    \n",
    "    #clustering model\n",
    "    gm=GaussianMixture(n_components=ncomp,n_init=100,max_iter=1000,random_state=0) \n",
    "    for i in range(iterN): #iterate\n",
    "#         print('Iteration {}'.format(i+1))  \n",
    "        clustering=gm.fit(y[rind,:]) #fit EM clustering model excluding outliers\n",
    "        l=clustering.score_samples(y) #estimate likelihood for each point\n",
    "        Lthres=sorted(l)[int(len(l)*pval)] #anomaly threshold\n",
    "        rind0=0+rind\n",
    "        rind=l>Lthres #non-anomalous points\n",
    "        if all(rind==rind0):\n",
    "#             print('Convergence in {} iterations'.format(i+1))\n",
    "            break\n",
    "    return l < Lthres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(city,maxComm=None):\n",
    "    f = open(RecordWritingPath+'F1ScoreSubCommunity.txt', 'a+')\n",
    "    print('Initialize')\n",
    "    df = readTransDF(TransportationDataPath,city)\n",
    "    lis_event,df_finalEvents = getEvents(EventDataPath,city)\n",
    "    print('Community Detection, aim at max', maxComm,' communities')\n",
    "    G = makeGraphfromDf(df)\n",
    "    partition = getComboPartition(G,comboPath,city)\n",
    "    df['start_community'] = df['start_id'].apply(lambda x: partition[str(x)])\n",
    "    df['end_community'] = df['end_id'].apply(lambda x: partition[str(x)])\n",
    "    Commdata = getSubCommunity(df)\n",
    "    print('aggregate date wise data by communities')\n",
    "    data = aggregateByCommunities(Commdata,city,TransportationDataPath)\n",
    "    print('Save Aggregated DF to csv')\n",
    "    data.to_csv(TransportationDataPath+'/CommDete/'+city+str(maxComm)+'Comm.csv',index=False)\n",
    "    dataTs = getTimeSeries(data)\n",
    "    matrix = dataTs.values\n",
    "    matrix = np.log(matrix+1)\n",
    "    \n",
    "    for i in range(matrix.shape[1]):\n",
    "        matrix[:, i] = (matrix[:, i] - matrix[:, i].min()) / (matrix[:, i].max() - matrix[:, i].min())\n",
    "    date = dataTs.index.to_frame().rename(columns={'date':'Date'})\n",
    "    threresult = {}\n",
    "    EventsDF = df_finalEvents['Date'].drop_duplicates().to_frame()\n",
    "    EventsDF['Anomaly'] = True\n",
    "    df = EventsDF.merge(date,on='Date',how='right')\n",
    "    df.fillna(False,inplace=True)\n",
    "    for comp in [1,2,3,4,5]:\n",
    "        print('n_component',comp)\n",
    "        for thres in range(1,10, 1):\n",
    "            th = thres/10\n",
    "    #         print(\"Threshhold: \",th)\n",
    "            outliers = anomalyDetection(matrix,comp,pval = th)\n",
    "            df['outlier'] = outliers\n",
    "\n",
    "            Precision = len(df[(df['outlier']==True)&(df['Anomaly']==True)])/len(df[df['outlier']==True])\n",
    "            Recall = len(df[(df['outlier']==True)&(df['Anomaly']==True)])/len(EventsDF)\n",
    "            if Precision+Recall > 0:\n",
    "                F1 = (2*Precision*Recall)/(Precision+Recall)\n",
    "            else:\n",
    "                F1 = 0\n",
    "            threresult[th] = F1\n",
    "        bestTh = max(threresult.items(), key=operator.itemgetter(1))[0]\n",
    "        print('best F1 score in',comp,' components is ',str(threresult[bestTh]))\n",
    "        #use best th to calculate again\n",
    "\n",
    "        outliers = anomalyDetection(matrix,comp,pval = bestTh)\n",
    "        df['outlier'] = outliers\n",
    "        TP = len(df[(df['outlier']==True)&(df['Anomaly']==True)])\n",
    "        FP = len(df[(df['outlier']==False)&(df['Anomaly']==True)])\n",
    "        TN = len(df[(df['outlier']==False)&(df['Anomaly']==False)])\n",
    "        FN = len(df[(df['outlier']==False)&(df['Anomaly']==True)])\n",
    "        if FP + TN > 0:\n",
    "            FPR = FP / (FP + TN)\n",
    "        else:\n",
    "            FPR = 0\n",
    "\n",
    "        f.write(city+',Comm + GMM,'+str(comp)+','+str(threresult[bestTh])+','+str(TP)+','+str(FP)+','+str(TN)+','+str(FN)+',')\n",
    "        for event in ['Global Event','Local Event']:\n",
    "#             print(event)\n",
    "            SingleEventDF = df_finalEvents[df_finalEvents['Type'] == event]\n",
    "            SingleEventDF = SingleEventDF.drop_duplicates()\n",
    "            SingleDF = date.merge(SingleEventDF, on='Date', how='left')\n",
    "        #     SingleDF = SingleEventDF.merge(date,on='Date',how='left')\n",
    "            SingleDF['outliers'] = outliers\n",
    "        #     SinglePrecision = len(SingleDF[(SingleDF['outlier']==True)&(SingleDF['Type'].notnull)])/len(SingleDF[SingleDF['outlier']==True])\n",
    "            SingleRecall = len(SingleDF[(SingleDF['outliers']==True)&(SingleDF['Type'].notnull())])/len(SingleEventDF)\n",
    "            SingleFPR = len(SingleDF[(SingleDF['outliers']==True)&(SingleDF['Type'].isna())])/len(SingleDF[SingleDF['Type'].isna()])\n",
    "            f.write(str(SingleRecall)+',')\n",
    "        f.write('\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_component 1\n",
      "n_component 2\n",
      "n_component 3\n",
      "n_component 4\n",
      "n_component 5\n"
     ]
    }
   ],
   "source": [
    "pipeline('DC',9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_component 1\n",
      "n_component 2\n",
      "n_component 3\n",
      "n_component 4\n",
      "n_component 5\n"
     ]
    }
   ],
   "source": [
    "pipeline('Taipei',10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_component 1\n",
      "n_component 2\n",
      "n_component 3\n",
      "n_component 4\n",
      "n_component 5\n"
     ]
    }
   ],
   "source": [
    "pipeline('Chicago',9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_component 1\n",
      "n_component 2\n",
      "n_component 3\n",
      "n_component 4\n",
      "n_component 5\n"
     ]
    }
   ],
   "source": [
    "pipeline('NewYork',6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
