{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from sklearn.metrics import r2_score\n",
    "pd.set_option('display.float_format', lambda x: '%.10f' % x)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import statsmodels.graphics.tsaplots as sg\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from sklearn.mixture import GaussianMixture \n",
    "pd.set_option('display.float_format', lambda x: '%.10f' % x)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "RecordWritingPath = '/Users/hemingyi/Documents/capstone/post/result/'\n",
    "TransportationDataPath = '/Users/hemingyi/Documents/capstone/post/transportation/'\n",
    "EventDataPath = '/Users/hemingyi/Documents/capstone/post/events/'\n",
    "comboPath = '/Users/hemingyi/Documents/capstone/combo/'\n",
    "PostData = '/Users/hemingyi/Documents/capstone/post/'\n",
    "# dataFile = TransportationDataPath+city+'EdgeYearwiseAggregated.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anomalyDetection(y,nc, pval = 0.2,iterN=20):\n",
    "    if len(y.shape) == 1:\n",
    "        y = np.array(y).reshape(-1,1)\n",
    "    rind = np.array(range(y.shape[0]))\n",
    "    #clustering model\n",
    "    gm=GaussianMixture(n_components=nc,n_init=100,max_iter=1000,random_state=0) \n",
    "    for i in range(iterN):\n",
    "#         print('Iteration {}'.format(i+1))  \n",
    "        clustering=gm.fit(y[rind,:]) #fit EM clustering model excluding outliers\n",
    "        l=clustering.score_samples(y) #estimate likelihood for each point\n",
    "        Lthres=sorted(l)[int(len(l)*pval)] #anomaly threshold\n",
    "        rind0=0+rind\n",
    "        rind=l>Lthres #non-anomalous points\n",
    "        if all(rind==rind0):\n",
    "#             print('Convergence in {} iterations'.format(i+1))\n",
    "            break\n",
    "    return l < Lthres\n",
    "\n",
    "\n",
    "def detectOutlier(df_residuals, df_finalEvents, nc):\n",
    "    df_results = pd.DataFrame(columns= ['Thres', 'F1'] + list(df_finalEvents['Type'].unique()) + ['tn', 'fp', 'fn', 'tp'])\n",
    "    eventsize = len(df_finalEvents['Date'].unique())/ len(df_residuals)\n",
    "    a = 0\n",
    "    for thres in [eventsize] + list(np.arange(0.1,1.0,0.1)):\n",
    "#         print(thres)\n",
    "        lis = [thres]\n",
    "        df_residuals['Outliers'] = anomalyDetection(df_residuals['residuals'],nc=nc,pval = thres)\n",
    "        lis.append(f1_score(df_residuals['true'], df_residuals['Outliers']))\n",
    "        for event in df_finalEvents['Type'].unique():\n",
    "            df_resTemp = df_residuals[df_residuals.index.isin(df_finalEvents[df_finalEvents['Type']==event]['Date'])]\n",
    "            lis.append(np.sum(df_resTemp['Outliers'])/len(df_resTemp))\n",
    "        lis = lis  + list(confusion_matrix(df_residuals['true'], df_residuals['Outliers']).ravel())\n",
    "        #print(lis)\n",
    "        df_results.loc[a,:] = lis\n",
    "        a = a+1\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DC\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 725 entries, 2016-01-01 to 2017-12-31\n",
      "Data columns (total 1 columns):\n",
      "amount    725 non-null int64\n",
      "dtypes: int64(1)\n",
      "memory usage: 11.3+ KB\n",
      "None\n",
      "Taipei\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 638 entries, 2017-01-01 to 2018-09-30\n",
      "Data columns (total 1 columns):\n",
      "amount    638 non-null float64\n",
      "dtypes: float64(1)\n",
      "memory usage: 10.0+ KB\n",
      "None\n",
      "NewYork\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 579 entries, 2017-06-01 to 2018-12-31\n",
      "Data columns (total 1 columns):\n",
      "amount    579 non-null int64\n",
      "dtypes: int64(1)\n",
      "memory usage: 9.0+ KB\n",
      "None\n",
      "Chicago\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 731 entries, 01/01/2015 to 12/31/2016\n",
      "Data columns (total 1 columns):\n",
      "amount    731 non-null float64\n",
      "dtypes: float64(1)\n",
      "memory usage: 11.4+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df_results = pd.DataFrame()\n",
    "for city in ['DC', 'Taipei', 'NewYork', 'Chicago']:\n",
    "    print(city)\n",
    "    subway_data = TransportationDataPath+city+'EdgeDatewiseAggregated.csv'\n",
    "    events_data = EventDataPath+city+'Events.csv'\n",
    "\n",
    "    # import data\n",
    "    df_main = pd.read_csv(subway_data)\n",
    "\n",
    "    # Aggregate on date\n",
    "    df_main = pd.DataFrame(df_main.groupby(['date'])['amount'].sum())\n",
    "\n",
    "    # sort by index\n",
    "    df_main = df_main.sort_index()\n",
    "    \n",
    "\n",
    "    # print details\n",
    "    print(df_main.info())\n",
    "\n",
    "    # change to datetime format\n",
    "    df_main.index = pd.to_datetime(df_main.index)\n",
    "#     print(df_main)\n",
    "#     break\n",
    "    # import events data\n",
    "    df_events = pd.read_csv(events_data, encoding = \"ISO-8859-1\", parse_dates=['Date'], infer_datetime_format=True)\n",
    "\n",
    "    # dataframe for events\n",
    "    df_finalEvents =  df_events[['Date', 'Type']]\n",
    "    df_finalEvents.index = pd.to_datetime(df_finalEvents.Date)\n",
    "    df_output = df_main.join(df_finalEvents, how='left')\n",
    "    del df_output['Date']\n",
    "#     df_output.to_csv(WebData+city+'DailyAggregated.csv')\n",
    "#     print(df_output)\n",
    "#     break\n",
    "\n",
    "    # list events\n",
    "    lis_event = df_finalEvents['Type'].unique()\n",
    "    lis_event = list(lis_event)\n",
    "\n",
    "    # fit model\n",
    "    model = ARIMA(df_main['amount'], order=(7,1,2))\n",
    "    model_fit = model.fit(disp=0)\n",
    "#     print(model_fit.summary())\n",
    "\n",
    "    # create residuals and predictions\n",
    "    df_residuals = pd.DataFrame(model_fit.resid, columns=['residuals'])\n",
    "\n",
    "    ## Original Anomalies\n",
    "    df_residuals['true'] = False\n",
    "    df_residuals.loc[df_residuals.index.isin(df_finalEvents['Date'].unique()), 'true'] = True\n",
    "\n",
    "    for i in range(1,6):\n",
    "        df_resultsTemp = detectOutlier(df_residuals, df_finalEvents, i)\n",
    "        df_resultsTemp['n_comp'] = i\n",
    "        df_resultsTemp['city'] = city\n",
    "        df_results = df_results.append(df_resultsTemp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.to_csv(RecordWritingPath+'TSResults.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
