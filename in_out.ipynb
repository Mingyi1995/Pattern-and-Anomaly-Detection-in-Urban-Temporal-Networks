{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.mixture import GaussianMixture \n",
    "from sklearn.decomposition import PCA\n",
    "import datetime\n",
    "import operator\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "RecordWritingPath = '/Users/hemingyi/Documents/capstone/code/result/web/'\n",
    "TransportationDataPath = '/Users/hemingyi/Documents/capstone/transportation/'\n",
    "EventDataPath = '/Users/hemingyi/Documents/capstone/event data/update/'\n",
    "comboPath = '/Users/hemingyi/Documents/capstone/combo/'\n",
    "WebData = '/Users/hemingyi/Documents/capstone/webdata/'\n",
    "# dataFile = TransportationDataPath+city+'EdgeYearwiseAggregated.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taipei\n",
      "done\n",
      "DC\n",
      "done\n",
      "Chicago\n",
      "done\n",
      "NewYork\n",
      "done\n",
      "all done\n"
     ]
    }
   ],
   "source": [
    "for city in ['Taipei','DC','Chicago','NewYork']:\n",
    "    print(city)\n",
    "    data = pd.read_csv(TransportationDataPath+city+'EdgeDatewiseAggregated.csv')\n",
    "    startId = data.groupby(['start_id','date']).sum()\n",
    "    endId = data.groupby(['end_id','date']).sum()\n",
    "    startPivot = pd.pivot_table(startId, values='amount', index=['date'],\n",
    "                        columns=['start_id'], aggfunc=sum, fill_value=0)\n",
    "    # startPivot.columns = [x+'start' for x in startPivot.columns]\n",
    "    endPivot = pd.pivot_table(endId, values='amount', index=['date'],\n",
    "                        columns=['end_id'], aggfunc=sum, fill_value=0)\n",
    "    # endPivot.columns = [x+'end' for x in endPivot.columns]\n",
    "    startPivot.to_csv(WebData+city+'inflow.csv')\n",
    "    endPivot.to_csv(WebData+city+'outflow.csv')\n",
    "    print('done')\n",
    "print('all done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taipei\n",
      "done\n",
      "DC\n",
      "done\n",
      "Chicago\n",
      "done\n",
      "NewYork\n",
      "done\n",
      "all done\n"
     ]
    }
   ],
   "source": [
    "for city in ['Taipei','DC','Chicago','NewYork']:\n",
    "    print(city)\n",
    "    data = pd.read_csv(TransportationDataPath+city+'EdgeDatewiseAggregated.csv')\n",
    "    startId = data.groupby(['start_id','date']).sum()\n",
    "    endId = data.groupby(['end_id','date']).sum()\n",
    "    startPivot = pd.pivot_table(startId, values='amount', index=['date'],\n",
    "                        columns=['start_id'], aggfunc=sum, fill_value=0)\n",
    "    startPivot.columns = [str(x)+'start' for x in startPivot.columns]\n",
    "    endPivot = pd.pivot_table(endId, values='amount', index=['date'],\n",
    "                        columns=['end_id'], aggfunc=sum, fill_value=0)\n",
    "    endPivot.columns = [str(x)+'end' for x in endPivot.columns]\n",
    "    InOutFlow = startPivot.merge(endPivot, on='date')\n",
    "    InOutFlow.to_csv(TransportationDataPath+city+'InOutFlow.csv',index=True)\n",
    "    print('done')\n",
    "print('all done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "startPivot = pd.pivot_table(startId, values='amount', index=['date'],\n",
    "                    columns=['start_id'], aggfunc=sum, fill_value=0)\n",
    "# startPivot.columns = [x+'start' for x in startPivot.columns]\n",
    "endPivot = pd.pivot_table(endId, values='amount', index=['date'],\n",
    "                    columns=['end_id'], aggfunc=sum, fill_value=0)\n",
    "# endPivot.columns = [x+'end' for x in endPivot.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anomalyDetection(y,ncom,pval = 0.2,iterN=20):\n",
    "    #index of regular (non-outlier points)\n",
    "\n",
    "    rind = np.array(range(y.shape[0]))\n",
    "    \n",
    "    #clustering model\n",
    "    gm=GaussianMixture(n_components=ncom,n_init=100,max_iter=1000,random_state=0) \n",
    "    for i in range(iterN): #iterate\n",
    "#         print('Iteration {}'.format(i+1))  \n",
    "        clustering=gm.fit(y[rind,:]) #fit EM clustering model excluding outliers\n",
    "        l=clustering.score_samples(y) #estimate likelihood for each point\n",
    "        Lthres=sorted(l)[int(len(l)*pval)] #anomaly threshold\n",
    "#         print(Lthres)\n",
    "        rind0=0+rind\n",
    "        rind=l>Lthres #non-anomalous points\n",
    "        if all(rind==rind0):\n",
    "#             print('Convergence in {} iterations'.format(i+1))\n",
    "            break\n",
    "    return l < Lthres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import events data\n",
    "def getEvents(EventDataPath,city):\n",
    "    events_data =EventDataPath+city+'Events.csv'\n",
    "    df_events = pd.read_csv(events_data, encoding = \"ISO-8859-1\", parse_dates=['Date'], infer_datetime_format=True)\n",
    "\n",
    "    # dataframe for events\n",
    "    df_finalEvents =  df_events[['Date', 'Type']]\n",
    "\n",
    "    # list events666\n",
    "    \n",
    "    lis_event = df_finalEvents['Type'].unique()\n",
    "    lis_event = list(lis_event)\n",
    "    return (lis_event,df_finalEvents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTimeSeries(df):\n",
    "    table = pd.pivot_table(df, values='amount', index=['date'],\n",
    "                    columns=['start_id','end_id'], aggfunc=np.sum, fill_value=0)\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anomalyDetection(y,ncomp,pval = 0.2,iterN=20):\n",
    "    #index of regular (non-outlier points)\n",
    "    #rind=y[:,0]>-10 \n",
    "    rind = np.array(range(y.shape[0]))\n",
    "    \n",
    "    #clustering model\n",
    "    gm=GaussianMixture(n_components=ncomp,n_init=100,max_iter=1000,random_state=0) \n",
    "    for i in range(iterN): #iterate\n",
    "#         print('Iteration {}'.format(i+1))  \n",
    "        clustering=gm.fit(y[rind,:]) #fit EM clustering model excluding outliers\n",
    "        l=clustering.score_samples(y) #estimate likelihood for each point\n",
    "        Lthres=sorted(l)[int(len(l)*pval)] #anomaly threshold\n",
    "        rind0=0+rind\n",
    "        rind=l>Lthres #non-anomalous points\n",
    "        if all(rind==rind0):\n",
    "#             print('Convergence in {} iterations'.format(i+1))\n",
    "            break\n",
    "    return l < Lthres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(city):\n",
    "    f = open(RecordWritingPath+'InOutFlowPCA.csv', 'a+')\n",
    "    print('Initialize')\n",
    "    lis_event,df_finalEvents = getEvents(EventDataPath,city)\n",
    "    data = pd.read_csv(TransportationDataPath+city+'InOutFlow.csv')\n",
    "    \n",
    "#     dataTs = getTimeSeries(data)\n",
    "#     dataTs = dataTs.sort_index()\n",
    "    matrix = data.drop(['date'], axis=1).values\n",
    "    matrix = np.log(matrix+1)\n",
    "    for i in range(matrix.shape[1]):\n",
    "        matrix[:, i] = (matrix[:, i] - matrix[:, i].min()) / (matrix[:, i].max() - matrix[:, i].min())\n",
    "    pca = PCA(n_components=16)\n",
    "    matrix=pca.fit_transform(matrix)\n",
    "    date = data.date.to_frame().rename(columns={'date':'Date'})\n",
    "#     print(date.head())\n",
    "    threresult = {}\n",
    "    EventsDF = df_finalEvents.drop_duplicates(subset='Date', keep='first', inplace=False)\n",
    "    EventsDF['Anomaly'] = True\n",
    "    EventsDF['Date'] = EventsDF['Date'].astype('str')\n",
    "    date['Date'] = date['Date'].astype('str')\n",
    "    df = date.merge(EventsDF,on='Date',how='left')\n",
    "    df.Anomaly.fillna(False, inplace=True)\n",
    "    for comp in [1,2,3,4,5]:\n",
    "        print('n_component',comp)\n",
    "        for thres in list(range(1,10,1))+[10*len(df[df['Anomaly']==True])/len(df)]:\n",
    "            th = thres/10\n",
    "            outliers = anomalyDetection(matrix,comp,pval = th)\n",
    "            df['outlier'] = outliers\n",
    "            TP = len(df[(df['outlier']==True)&(df['Anomaly']==True)])\n",
    "            FP = len(df[(df['outlier']==True)&(df['Anomaly']==False)])\n",
    "            TN = len(df[(df['outlier']==False)&(df['Anomaly']==False)])\n",
    "            FN = len(df[(df['outlier']==False)&(df['Anomaly']==True)])\n",
    "            f.write(city+',Comm + PCA + GMM,'+str(comp)+','+str(th)+','+str(TP)+','+str(FP)+','+str(TN)+','+str(FN)+',')\n",
    "            for event in ['National Holiday', 'Culture Event', 'Extreme Weather']:\n",
    "    #             print(event)\n",
    "                T = len(df[df['Type']==event])\n",
    "                TP = len(df[(df['Type']==event)&(df['outlier']==True)])\n",
    "                TPR = TP/T\n",
    "                f.write(str(TPR)+',')\n",
    "            f.write('\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NewYork\n",
      "Initialize\n",
      "saved pca matrix\n",
      "n_component 1\n",
      "n_component 2\n",
      "n_component 3\n",
      "n_component 4\n",
      "n_component 5\n"
     ]
    }
   ],
   "source": [
    "for city in ['NewYork']:\n",
    "    print(city)\n",
    "    pipeline(city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize\n"
     ]
    }
   ],
   "source": [
    "city = 'NewYork'\n",
    "print('Initialize')\n",
    "lis_event,df_finalEvents = getEvents(EventDataPath,city)\n",
    "data = pd.read_csv(TransportationDataPath+city+'InOutFlow.csv')\n",
    "\n",
    "#     dataTs = getTimeSeries(data)\n",
    "#     dataTs = dataTs.sort_index()\n",
    "matrix = data.drop(['date'], axis=1).values\n",
    "matrix = np.log(matrix+1)\n",
    "for i in range(matrix.shape[1]):\n",
    "    matrix[:, i] = (matrix[:, i] - matrix[:, i].min()) / (matrix[:, i].max() - matrix[:, i].min())\n",
    "pca = PCA(n_components=16)\n",
    "matrix=pca.fit_transform(matrix)\n",
    "date = data.date.to_frame().rename(columns={'date':'Date'})\n",
    "#     print(date.head())\n",
    "threresult = {}\n",
    "EventsDF = df_finalEvents.drop_duplicates(subset='Date', keep='first', inplace=False)\n",
    "EventsDF['Anomaly'] = True\n",
    "EventsDF['Date'] = EventsDF['Date'].astype('str')\n",
    "date['Date'] = date['Date'].astype('str')\n",
    "df = date.merge(EventsDF,on='Date',how='left')\n",
    "df.Anomaly.fillna(False, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan], dtype=object)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Type.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Type</th>\n",
       "      <th>Anomaly</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>National Holiday</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-12-25</td>\n",
       "      <td>National Holiday</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-11-22</td>\n",
       "      <td>National Holiday</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-11-12</td>\n",
       "      <td>National Holiday</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-09-03</td>\n",
       "      <td>National Holiday</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date              Type  Anomaly\n",
       "0  2017-01-02  National Holiday     True\n",
       "1  2018-12-25  National Holiday     True\n",
       "2  2018-11-22  National Holiday     True\n",
       "3  2018-11-12  National Holiday     True\n",
       "4  2018-09-03  National Holiday     True"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EventsDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv(TransportationDataPath+city+'InOutFlow.csv')\n",
    "dateparse = lambda x: pd.datetime.strptime(x, '%m/%d/%Y')\n",
    "data = pd.read_csv(TransportationDataPath+city+'InOutFlow.csv', parse_dates=['date'], date_parser=dateparse)\n",
    "data.to_csv(TransportationDataPath+city+'InOutFlow.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
