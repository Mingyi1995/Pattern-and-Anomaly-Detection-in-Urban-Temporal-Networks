{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#add necessary libraries\n",
    "import networkx as nx #library supporting networks\n",
    "import matplotlib.pyplot as plt #plotting\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stat\n",
    "from scipy import optimize\n",
    "#make sure plots are embedded into the notebook\n",
    "%pylab inline \n",
    "import statsmodels.formula.api as smf\n",
    "#import shapefile as shp\n",
    "#from shapely.geometry.polygon import Polygon\n",
    "from descartes import PolygonPatch\n",
    "import os\n",
    "from networkx.algorithms import community\n",
    "from sklearn.mixture import GaussianMixture \n",
    "\n",
    "# Import required libraries\n",
    "import keras\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, Flatten, Reshape, Dropout\n",
    "from keras import regularizers\n",
    "from keras import optimizers\n",
    "import pickle\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tensorflow import set_random_seed\n",
    "from keras import backend as K\n",
    "import datetime\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "import operator\n",
    "import json\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RecordWritingPath = '/Users/hemingyi/Documents/capstone/Synthetic_data/'\n",
    "DataPath = '/Users/hemingyi/Documents/capstone/Synthetic_data/'\n",
    "comboPath = '/Users/hemingyi/Documents/capstone/combo/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TransportationDataPath = '/Users/hemingyi/Documents/capstone/transportation/'\n",
    "SpatialDataPath = '/Users/hemingyi/Documents/capstone/spatial/'\n",
    "EventDataPath = '/Users/hemingyi/Documents/capstone/event data/new/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnInfo = pd.read_csv('columnInfo.csv')\n",
    "columnInfo['generated'] = columnInfo['Unnamed: 0'].apply(lambda x: 'station' + str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = columnInfo.append([columnInfo]*9999,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "synNP = np.load(DataPath+'syntheticGlobalAnomaly.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn = pd.DataFrame(data=synNP)\n",
    "syn['date'] = syn.index\n",
    "name = columnInfo['generated'].tolist() + ['date']\n",
    "syn.columns = name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"annually\" data\n",
    "ann = syn.sum()\n",
    "ann = pd.DataFrame(data=ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station</th>\n",
       "      <th>amount</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>start_id</th>\n",
       "      <th>end_id</th>\n",
       "      <th>generated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>station0</td>\n",
       "      <td>5.290540e+05</td>\n",
       "      <td>0</td>\n",
       "      <td>BL01</td>\n",
       "      <td>BL01</td>\n",
       "      <td>station0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>station1</td>\n",
       "      <td>1.523296e+06</td>\n",
       "      <td>1</td>\n",
       "      <td>BL01</td>\n",
       "      <td>BL02</td>\n",
       "      <td>station1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>station2</td>\n",
       "      <td>1.288300e+06</td>\n",
       "      <td>2</td>\n",
       "      <td>BL01</td>\n",
       "      <td>BL03</td>\n",
       "      <td>station2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>station3</td>\n",
       "      <td>2.434376e+06</td>\n",
       "      <td>3</td>\n",
       "      <td>BL01</td>\n",
       "      <td>BL04</td>\n",
       "      <td>station3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>station4</td>\n",
       "      <td>2.084335e+06</td>\n",
       "      <td>4</td>\n",
       "      <td>BL01</td>\n",
       "      <td>BL05</td>\n",
       "      <td>station4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    station        amount  Unnamed: 0 start_id end_id generated\n",
       "0  station0  5.290540e+05           0     BL01   BL01  station0\n",
       "1  station1  1.523296e+06           1     BL01   BL02  station1\n",
       "2  station2  1.288300e+06           2     BL01   BL03  station2\n",
       "3  station3  2.434376e+06           3     BL01   BL04  station3\n",
       "4  station4  2.084335e+06           4     BL01   BL05  station4"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TransformAnn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann['station'] = ann.index\n",
    "ann.columns = ['amount','station']\n",
    "ann = ann[['station','amount']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "TransformAnn = ann.merge(columnInfo,left_on='station',right_on='generated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "## transform to the transportation data format\n",
    "transform = pd.wide_to_long(syn, ['station'], i=\"date\", j=\"stationame\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformDF = transform.merge(columnInfo,left_on='stationame',right_on='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "TransformAnn = TransformAnn[['amount','start_id','end_id']]\n",
    "TransformAnn.to_csv('TaipeiAnnuallySynthetic.csv',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformDF = transformDF[['station','start_id','end_id']]\n",
    "transformDF['date'] = transformDF.index\n",
    "transformDF.columns = ['amount','start_id','end_id','date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformDF.to_csv('TaipeiDateWiseSynthetic.csv',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = [5048, 2337, 7935, 9157, 9339, 5923, 6570, 7134, 6580, 3196, 4943, 7458, 2159, 2482, 1401, 463, 8227, 4553, 1028, 4324, 1851, 8351, 3047, 697, 628, 1332, 9297, 6616, 4510, 9871, 8898, 1765, 7267, 7310, 4005, 2731, 6373, 615, 681, 9190, 1897, 9724, 6830, 3523, 913, 5043, 4290, 385, 6822, 1512, 7902, 3078, 6333, 9577, 77, 5236, 6593, 8246, 7518, 5064, 642, 6995, 5525, 4289, 1595, 4662, 4731, 9146, 4043, 8050, 8411, 8185, 3985, 2837, 7087, 1950, 7912, 6896, 6398, 6275, 2051, 3488, 2311, 8225, 3600, 3002, 9089, 9615, 8664, 1018, 8897, 6234, 6806, 4081, 9598, 7213, 6181, 705, 394, 1618, 347, 7793, 4608, 7602, 5636, 3988, 5049, 3004, 6924, 5280, 6092, 1656, 7878, 2991, 9848, 7296, 7112, 6365, 7648, 3940, 3588, 9760, 7251, 6205, 9822, 447, 9163, 9115, 2855, 7120, 2236, 9448, 591, 6108, 4265, 3098, 3522, 4398, 4338, 870, 8727, 7630, 2416, 4048, 7381, 4225, 1214, 7262, 5538, 8622, 7606, 7736, 9863, 9400, 3305, 126, 5660, 1494, 3743, 5434, 7050, 9564, 6048, 9298, 3921, 4968, 1404, 9508, 2096, 856, 8576, 4220, 7188, 2457, 477, 4625, 747, 4055, 3694, 3236, 6164, 6938, 5747, 2981, 5162, 8252, 3740, 4932, 2196, 9426, 9904, 109, 5922, 5099, 1551, 8907, 4232, 4184, 9000, 99, 2958, 730, 7864, 848, 5871, 5804, 1002, 3263, 9165, 9902, 1593, 5704, 9418, 4769, 7239, 9859, 4847, 9119, 1819, 6416, 6019, 3310, 1784, 4876, 5542, 8113, 7473, 344, 1643, 3461, 5347, 5159, 1955, 8947, 589, 9771, 2504, 1884, 216, 2149, 871, 801, 6413, 7943, 7330, 4560, 7475, 7196, 5981, 2305, 4777, 860, 9414, 1399, 3430, 1496, 2561, 6689, 4915, 2908, 8833, 3369, 8955, 8319, 1833, 4877, 7249, 148, 4688, 4092, 3998, 6539, 4969, 1647, 3966, 1381, 4296, 5314, 6039, 5326, 843, 896, 5832, 1216, 2544, 7701, 6473, 2722, 6392, 6489, 7268, 3502, 5932, 6509, 2604, 2700, 8450, 7447, 8788, 1564, 4278, 3160, 9466, 4079, 6285, 1761, 5419, 4064, 4776, 9838, 3786, 104, 2299, 4991, 932, 6236, 9527, 4617, 3325, 7929, 5941, 8322, 4727, 2910, 2372, 2820, 5607, 5661, 9488, 4887, 7107, 8163, 9120, 6468, 6228, 190, 5176, 6745, 5900, 6606, 2015, 5005, 3515, 6334, 8754, 4136, 4787, 8781, 2934, 3862, 909, 9128, 4535, 5905, 757, 6573, 127, 8666, 1375, 6563, 7571, 3480, 4363, 929, 6902, 2798, 7456, 6831, 9750, 5379, 865, 9670, 9836, 5393, 9230, 93, 9443, 781, 6245, 3438, 2992, 7796, 9735, 1268, 1527, 3738, 895, 3517, 5447, 4365, 6319, 696, 6369, 4916, 188, 265, 8957, 7953, 9464, 2469, 6451, 5999, 9559, 1032, 6854, 8211, 9736, 9755, 4201, 867, 9923, 8303, 7365, 2523, 9725, 4957, 3005, 8308, 8334, 7579, 7115, 8518, 6377, 1918, 4687, 4567, 7044, 4442, 5554, 1240, 7109, 6362, 1286, 1693, 8, 3886, 6322, 3073, 3701, 209, 5220, 2580, 2499, 4268, 7375, 7082, 304, 5523, 8595, 3378, 7823, 8601, 3635, 926, 5545, 1164, 5238, 8786, 4578, 7233, 6132, 302, 9850, 3734, 9324, 4045, 5125, 382, 105, 2518, 1518, 6566, 5062, 8102, 9654, 2248, 2277, 4461, 7567, 529, 4359, 5155, 3061, 2094, 1483, 5387, 7246, 9392, 1034, 4387, 1283, 7916, 4529, 7913, 2192, 3009, 9396, 75, 4914, 5611]\n",
    "EventDFTrans = pd.DataFrame()\n",
    "EventDFTrans['Date'] = events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readTransDF(TransportationDataPath,city):\n",
    "    df = pd.read_csv(TransportationDataPath+city+'EdgeYearwiseAggregated.csv')\n",
    "    return df\n",
    "def makeGraphfromDf(df):\n",
    "    G=nx.DiGraph()\n",
    "    nx.set_edge_attributes(G,'weight', 0)\n",
    "    for k in df.index:\n",
    "        G.add_edge(df['start_id'][k],df['end_id'][k],weight=df['amount'][k])\n",
    "#     nx.write_edgelist(G, comboPath+'temp/%s.net'%city)\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getComboPartition(G,maxcom,comboPath,city):\n",
    "    #save network in net format\n",
    "    nodenum={}\n",
    "#     G = makeGraphfromDf(df)\n",
    "    #create a dictionary transforming nodes to unique numbers\n",
    "    nodes = list(G.nodes())\n",
    "    print('nodes amount: ',len(nodes))\n",
    "    for i,j in enumerate(list(G.nodes())):\n",
    "        nodenum[str(j)] = str(i)\n",
    "#         nodes[str(i)] = str(j)\n",
    "#     i=0\n",
    "#     for n in list(G.nodes()):\n",
    "#         nodenum[n]=i\n",
    "#         nodes[i]=n\n",
    "#         i+=1\n",
    "\n",
    "    tempNetFile = comboPath+'temp/%s.net'%city\n",
    "    f = open(tempNetFile, 'w')\n",
    "    f.write('*Arcs\\n')\n",
    "\n",
    "    for e in G.edges(data=True):\n",
    "        f.write('{0} {1} {2}\\n'.format(nodenum[str(e[0])],nodenum[str(e[1])],e[2]['weight']))\n",
    "    f.close()\n",
    "\n",
    "    #run combo\n",
    "    command= comboPath+'/comboCPP '+tempNetFile#+' '+str(maxcom)\n",
    "    os.system(command)\n",
    "\n",
    "    #read resulting partition\n",
    "    partitionFile = comboPath+'temp/'+city + '_comm_comboC++.txt'\n",
    "    f = open(partitionFile, 'r')\n",
    "    i=0\n",
    "    partition={}\n",
    "    for line in f:\n",
    "        partition[str(nodes[i])]=str(int(line))\n",
    "        i+=1\n",
    "#         print(i)\n",
    "    f.close()\n",
    "    os.remove(partitionFile) \n",
    "    os.remove(tempNetFile)\n",
    "#     print(\"Communities: \",len(set(partition.values())))\n",
    "    return partition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSubCommunity(partition,df):\n",
    "    df['start_community'] = df['start_id'].apply(lambda x: partition[str(x)])\n",
    "    df['end_community'] = df['end_id'].apply(lambda x: partition[str(x)])\n",
    "    data2 = df[df.start_community == df.end_community]\n",
    "    communities = list(set(df.start_community))\n",
    "    print('communities numbers: ',communities)\n",
    "    SubPartition = {}\n",
    "    for c in communities:\n",
    "        print('detecting sub-communities in community ',c)\n",
    "        d = data2[data2.start_community == c]\n",
    "        graph = makeGraphfromDf(d)\n",
    "        p = getComboPartition(graph,maxcom,comboPath,city)\n",
    "        SubPartition[c] = p\n",
    "    return (SubPartition,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregateByCommunities(partition,SubPartition,data):\n",
    "\n",
    "\n",
    "    print('Raw shape: ',data.shape)\n",
    "    if 'Date' in data.columns:\n",
    "        data['date'] = pd.to_datetime(data.Date)\n",
    "    elif 'date' in data.columns:\n",
    "        data['date'] = pd.to_datetime(data.date)\n",
    "    else:\n",
    "        print(data.columns)\n",
    "\n",
    "    data['start_community'] = data['start_id'].apply(lambda x: partition[str(x)])\n",
    "    data['end_community'] = data['end_id'].apply(lambda x: partition[str(x)])\n",
    "    data['subStart'] = data.apply(lambda x: str(x['start_community'])+'.'+str(SubPartition[x['start_community']][str(x['start_id'])]),axis=1)\n",
    "    data['subEnd'] = data.apply(lambda x: str(x['end_community'])+'.'+str(SubPartition[x['end_community']][str(x['end_id'])]),axis=1)\n",
    "\n",
    "    communityData = data[['subStart','subEnd','date','amount']]\n",
    "    communityData = communityData.groupby(['subStart','subEnd','date']).sum().reset_index()\n",
    "    communityData.columns = ['start_id', 'end_id', 'date', 'amount']\n",
    "\n",
    "    return communityData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTimeSeries(df):\n",
    "    table = pd.pivot_table(df, values='amount', index=['date'],\n",
    "                    columns=['start_id','end_id'], aggfunc=np.sum, fill_value=0)\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anomalyDetection(y,pval = 0.2,iterN=20):\n",
    "    #index of regular (non-outlier points)\n",
    "\n",
    "    rind = np.array(range(y.shape[0]))\n",
    "    \n",
    "    #clustering model\n",
    "    gm=GaussianMixture(n_components=3,n_init=100,max_iter=1000,random_state=0) \n",
    "    for i in range(iterN): #iterate\n",
    "        print('Iteration {}'.format(i+1))  \n",
    "        clustering=gm.fit(y[rind,:]) #fit EM clustering model excluding outliers\n",
    "        l=clustering.score_samples(y) #estimate likelihood for each point\n",
    "        Lthres=sorted(l)[int(len(l)*pval)] #anomaly threshold\n",
    "        rind0=0+rind\n",
    "        rind=l>Lthres #non-anomalous points\n",
    "        if all(rind==rind0):\n",
    "            print('Convergence in {} iterations'.format(i+1))\n",
    "            break\n",
    "    return l < Lthres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import events data\n",
    "def getEvents(EventDataPath,city):\n",
    "    events_data =EventDataPath+city+'Events.csv'\n",
    "    df_events = pd.read_csv(events_data, encoding = \"ISO-8859-1\", parse_dates=['Date'], infer_datetime_format=True)\n",
    "\n",
    "    # dataframe for events\n",
    "    df_finalEvents =  df_events[['Date', 'Type']]\n",
    "\n",
    "    # list events666\n",
    "    \n",
    "    lis_event = df_finalEvents['Type'].unique()\n",
    "    lis_event = list(lis_event)\n",
    "    return (lis_event,df_finalEvents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-cf2c33096f27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# lis_event,df_finalEvents = getEvents(EventDataPath,city)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdf_finalEvents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEventDFTrans\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmakeGraphfromDf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mpartition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetComboPartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmaxcom\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcomboPath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mSubPartition\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetSubCommunity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-09f4c777ef63>\u001b[0m in \u001b[0;36mmakeGraphfromDf\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_edge_attributes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'weight'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_edge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'start_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'end_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'amount'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;31m#     nx.write_edgelist(G, comboPath+'temp/%s.net'%city)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 767\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   3116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3117\u001b[0m             return self._engine.get_value(s, k,\n\u001b[0;32m-> 3118\u001b[0;31m                                           tz=getattr(series.dtype, 'tz', None))\n\u001b[0m\u001b[1;32m   3119\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3120\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minferred_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'integer'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'boolean'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "f = open(RecordWritingPath+'F1ScoreSubCommunity.txt', 'w+')\n",
    "f.write('Sub Community Detection\\n')\n",
    "f.write('Taipei Synthetic Global Events'+'\\n')\n",
    "\n",
    "df = pd.read_csv('TaipeiAnnuallySynthetic.csv')\n",
    "# lis_event,df_finalEvents = getEvents(EventDataPath,city)\n",
    "df_finalEvents = EventDFTrans\n",
    "G = makeGraphfromDf(df)\n",
    "partition = getComboPartition(G,maxcom,comboPath,city)\n",
    "SubPartition,df = getSubCommunity(partition,df)\n",
    "datewiseDF = pd.read_csv('TaipeiDateWiseSynthetic.csv')\n",
    "data = aggregateByCommunities(partition,SubPartition,data=datewiseDF)\n",
    "dataTs = getTimeSeries(data)\n",
    "matrix = dataTs.values\n",
    "matrix = np.log(matrix+1)\n",
    "for i in range(matrix.shape[1]):\n",
    "    matrix[:, i] = (matrix[:, i] - matrix[:, i].min()) / (matrix[:, i].max() - matrix[:, i].min())\n",
    "date = dataTs.index.to_frame().rename(columns={'date':'Date'})\n",
    "result = {}\n",
    "EventsDF = df_finalEvents['Date'].drop_duplicates().to_frame()\n",
    "EventsDF['Anomaly'] = True\n",
    "df = EventsDF.merge(date,on='Date',how='right')\n",
    "for thres in range(1,10, 1):\n",
    "    th = thres/10\n",
    "    print(\"Threshhold: \",th)\n",
    "    outliers = anomalyDetection(matrix,pval = th)\n",
    "    df['outlier'] = outliers\n",
    "    TP = len(df[(df['outlier']==True)&(df['Anomaly']==True)])\n",
    "    FP = len(df[(df['outlier']==False)&(df['Anomaly']==True)])\n",
    "    TN = len(df[(df['outlier']==False)&(df['Anomaly']==False)])\n",
    "    FN = len(df[(df['outlier']==False)&(df['Anomaly']==True)])\n",
    "\n",
    "    Precision = len(df[(df['outlier']==True)&(df['Anomaly']==True)])/len(df[df['outlier']==True])\n",
    "    Recall = len(df[(df['outlier']==True)&(df['Anomaly']==True)])/len(EventsDF)\n",
    "    if Precision+Recall > 0:\n",
    "        F1 = (2*Precision*Recall)/(Precision+Recall)\n",
    "    else:\n",
    "        F1 = 0\n",
    "    result[th] = F1\n",
    "F1df = pd.DataFrame.from_dict(data)\n",
    "F1df['city'] = city\n",
    "F1df.to_csv(RecordWritingPath+'F1scoreAllRecords.csv', mode='a+',hearder=False)\n",
    "bestTh = max(result.items(), key=operator.itemgetter(1))[0]\n",
    "\n",
    "#use best th to calculate again\n",
    "outliers = anomalyDetection(matrix,pval = bestTh)\n",
    "df['outlier'] = outliers\n",
    "TP = len(df[(df['outlier']==True)&(df['Anomaly']==True)])\n",
    "FP = len(df[(df['outlier']==False)&(df['Anomaly']==True)])\n",
    "TN = len(df[(df['outlier']==False)&(df['Anomaly']==False)])\n",
    "FN = len(df[(df['outlier']==False)&(df['Anomaly']==True)])\n",
    "\n",
    "f.wrtie('TP: '+str(TP)+' FP: '+str(FP)+' TN: '+str(TN)+' FN: '+str(FN)+'\\n')\n",
    "f.write('overall F1: '+str(result[bestTh])+', Threshold: '+str(bestTh)+'\\n')\n",
    "f.write('Events amount: '+str(len(EventDFTrans))+' Days: '+str(len(datewiseDF))+' ratio: '+str(len(EventDFTrans)/len(datewiseDF)))\n",
    "f.write('event,single_type_recall,single_type_FPR\\n')\n",
    "EventType = list(df_finalEvents['Type'].unique())\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116640000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
