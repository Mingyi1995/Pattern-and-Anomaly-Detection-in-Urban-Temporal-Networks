{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#add necessary libraries\n",
    "import networkx as nx #library supporting networks\n",
    "import matplotlib.pyplot as plt #plotting\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stat\n",
    "from scipy import optimize\n",
    "#make sure plots are embedded into the notebook\n",
    "%pylab inline \n",
    "import statsmodels.formula.api as smf\n",
    "#import shapefile as shp\n",
    "#from shapely.geometry.polygon import Polygon\n",
    "from descartes import PolygonPatch\n",
    "import os\n",
    "from networkx.algorithms import community\n",
    "from sklearn.mixture import GaussianMixture \n",
    "\n",
    "# Import required libraries\n",
    "import keras\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, Flatten, Reshape, Dropout\n",
    "from keras import regularizers\n",
    "from keras import optimizers\n",
    "import pickle\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tensorflow import set_random_seed\n",
    "from keras import backend as K\n",
    "import datetime\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "import operator\n",
    "import json\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "RecordWritingPath = '/Users/hemingyi/Documents/capstone/Synthetic_data/'\n",
    "DataPath = '/Users/hemingyi/Documents/capstone/Synthetic_data/'\n",
    "comboPath = '/Users/hemingyi/Documents/capstone/combo/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnInfo = pd.read_csv('columnInfo.csv')\n",
    "columnInfo['generated'] = columnInfo['Unnamed: 0'].apply(lambda x: 'station' + str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = columnInfo.append([columnInfo]*9999,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synNP = np.load(DataPath+'syntheticGlobalAnomaly.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn = pd.DataFrame(data=synNP)\n",
    "syn['date'] = syn.index\n",
    "name = columnInfo['generated'].tolist() + ['date']\n",
    "syn.columns = name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"annually\" data\n",
    "ann = syn.sum()\n",
    "ann = pd.DataFrame(data=ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TransformAnn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann['station'] = ann.index\n",
    "ann.columns = ['amount','station']\n",
    "ann = ann[['station','amount']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TransformAnn = ann.merge(columnInfo,left_on='station',right_on='generated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## transform to the transportation data format\n",
    "transform = pd.wide_to_long(syn, ['station'], i=\"date\", j=\"stationame\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformDF = transform.merge(columnInfo,left_on='stationame',right_on='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TransformAnn = TransformAnn[['amount','start_id','end_id']]\n",
    "TransformAnn.to_csv('TaipeiAnnuallySynthetic.csv',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformDF = transformDF[['station','start_id','end_id']]\n",
    "transformDF['date'] = transformDF.index\n",
    "transformDF.columns = ['amount','start_id','end_id','date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformDF.to_csv('TaipeiDateWiseSynthetic.csv',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readTransDF(TransportationDataPath,city):\n",
    "    df = pd.read_csv(TransportationDataPath+city+'EdgeYearwiseAggregated.csv')\n",
    "    return df\n",
    "def makeGraphfromDf(df):\n",
    "    G=nx.DiGraph()\n",
    "    nx.set_edge_attributes(G,'weight', 0)\n",
    "    for k in df.index:\n",
    "        G.add_edge(df['start_id'][k],df['end_id'][k],weight=df['amount'][k])\n",
    "#     nx.write_edgelist(G, comboPath+'temp/%s.net'%city)\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getComboPartition(G,comboPath,city):\n",
    "    #save network in net format\n",
    "    nodenum={}\n",
    "#     G = makeGraphfromDf(df)\n",
    "    #create a dictionary transforming nodes to unique numbers\n",
    "    nodes = list(G.nodes())\n",
    "    print('nodes amount: ',len(nodes))\n",
    "    for i,j in enumerate(list(G.nodes())):\n",
    "        nodenum[str(j)] = str(i)\n",
    "#         nodes[str(i)] = str(j)\n",
    "#     i=0\n",
    "#     for n in list(G.nodes()):\n",
    "#         nodenum[n]=i\n",
    "#         nodes[i]=n\n",
    "#         i+=1\n",
    "\n",
    "    tempNetFile = comboPath+'temp/%s.net'%city\n",
    "    f = open(tempNetFile, 'w')\n",
    "    f.write('*Arcs\\n')\n",
    "\n",
    "    for e in G.edges(data=True):\n",
    "        f.write('{0} {1} {2}\\n'.format(nodenum[str(e[0])],nodenum[str(e[1])],e[2]['weight']))\n",
    "    f.close()\n",
    "\n",
    "    #run combo\n",
    "    command= comboPath+'/comboCPP '+tempNetFile#+' '+str(maxcom)\n",
    "    os.system(command)\n",
    "\n",
    "    #read resulting partition\n",
    "    partitionFile = comboPath+'temp/'+city + '_comm_comboC++.txt'\n",
    "    f = open(partitionFile, 'r')\n",
    "    i=0\n",
    "    partition={}\n",
    "    for line in f:\n",
    "        partition[str(nodes[i])]=str(int(line))\n",
    "        i+=1\n",
    "#         print(i)\n",
    "    f.close()\n",
    "    os.remove(partitionFile) \n",
    "    os.remove(tempNetFile)\n",
    "#     print(\"Communities: \",len(set(partition.values())))\n",
    "    return partition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSubCommunity(partition,df,city):\n",
    "    df['start_community'] = df['start_id'].apply(lambda x: partition[x])\n",
    "    df['end_community'] = df['end_id'].apply(lambda x: partition[x])\n",
    "    data2 = df[df.start_community == df.end_community]\n",
    "    communities = list(set(df.start_community))\n",
    "    print('communities numbers: ',communities)\n",
    "    SubPartition = {}\n",
    "    for c in communities:\n",
    "        print('detecting sub-communities in community ',c)\n",
    "        d = data2[data2.start_community == c]\n",
    "        graph = makeGraphfromDf(d)\n",
    "        p = getComboPartition(graph,comboPath,city)\n",
    "        SubPartition[c] = p\n",
    "    return (SubPartition,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['start_community'] = df['start_id'].apply(lambda x: partition[str(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nodes amount:  108\n",
      "communities numbers:  ['1', '3', '2', '0']\n",
      "detecting sub-communities in community  1\n",
      "nodes amount:  23\n",
      "detecting sub-communities in community  3\n",
      "nodes amount:  21\n",
      "detecting sub-communities in community  2\n",
      "nodes amount:  40\n",
      "detecting sub-communities in community  0\n",
      "nodes amount:  24\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('TaipeiAnnuallySynthetic.csv')\n",
    "G = makeGraphfromDf(df)\n",
    "partition = getComboPartition(G,comboPath,'TaipeiSyn')\n",
    "SubPartition,df = getSubCommunity(partition, df,'TaipeiSyn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('partition.json', 'w') as fp:\n",
    "    json.dump(partition, fp)\n",
    "\n",
    "with open('SubPartition.json', 'w') as fp:\n",
    "    json.dump(SubPartition, fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregateByCommunities(partition,SubPartition,data):\n",
    "\n",
    "\n",
    "    print('Raw shape: ',data.shape)\n",
    "    if 'Date' in data.columns:\n",
    "        data['date'] = pd.to_datetime(data.Date)\n",
    "    elif 'date' in data.columns:\n",
    "        data['date'] = pd.to_datetime(data.date)\n",
    "    else:\n",
    "        print(data.columns)\n",
    "\n",
    "    data['start_community'] = data['start_id'].apply(lambda x: partition[str(x)])\n",
    "    data['end_community'] = data['end_id'].apply(lambda x: partition[str(x)])\n",
    "    data['subStart'] = data.apply(lambda x: str(x['start_community'])+'.'+str(SubPartition[x['start_community']][str(x['start_id'])]),axis=1)\n",
    "    data['subEnd'] = data.apply(lambda x: str(x['end_community'])+'.'+str(SubPartition[x['end_community']][str(x['end_id'])]),axis=1)\n",
    "\n",
    "    communityData = data[['subStart','subEnd','date','amount']]\n",
    "    communityData = communityData.groupby(['subStart','subEnd','date']).sum().reset_index()\n",
    "    communityData.columns = ['start_id', 'end_id', 'date', 'amount']\n",
    "\n",
    "    return communityData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTimeSeries(df):\n",
    "    table = pd.pivot_table(df, values='amount', index=['date'],\n",
    "                    columns=['start_id','end_id'], aggfunc=np.sum, fill_value=0)\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anomalyDetection(y,pval = 0.2,iterN=20):\n",
    "    #index of regular (non-outlier points)\n",
    "\n",
    "    rind = np.array(range(y.shape[0]))\n",
    "    \n",
    "    #clustering model\n",
    "    gm=GaussianMixture(n_components=3,n_init=100,max_iter=1000,random_state=0) \n",
    "    for i in range(iterN): #iterate\n",
    "        print('Iteration {}'.format(i+1))  \n",
    "        clustering=gm.fit(y[rind,:]) #fit EM clustering model excluding outliers\n",
    "        l=clustering.score_samples(y) #estimate likelihood for each point\n",
    "        Lthres=sorted(l)[int(len(l)*pval)] #anomaly threshold\n",
    "        rind0=0+rind\n",
    "        rind=l>Lthres #non-anomalous points\n",
    "        if all(rind==rind0):\n",
    "            print('Convergence in {} iterations'.format(i+1))\n",
    "            break\n",
    "    return l < Lthres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import events data\n",
    "def getEvents(EventDataPath,city):\n",
    "    events_data =EventDataPath+city+'Events.csv'\n",
    "    df_events = pd.read_csv(events_data, encoding = \"ISO-8859-1\", parse_dates=['Date'], infer_datetime_format=True)\n",
    "\n",
    "    # dataframe for events\n",
    "    df_finalEvents =  df_events[['Date', 'Type']]\n",
    "\n",
    "    # list events666\n",
    "    \n",
    "    lis_event = df_finalEvents['Type'].unique()\n",
    "    lis_event = list(lis_event)\n",
    "    return (lis_event,df_finalEvents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "city = 'SyntheticTaipei'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = [5048, 2337, 7935, 9157, 9339, 5923, 6570, 7134, 6580, 3196, 4943, 7458, 2159, 2482, 1401, 463, 8227, 4553, 1028, 4324, 1851, 8351, 3047, 697, 628, 1332, 9297, 6616, 4510, 9871, 8898, 1765, 7267, 7310, 4005, 2731, 6373, 615, 681, 9190, 1897, 9724, 6830, 3523, 913, 5043, 4290, 385, 6822, 1512, 7902, 3078, 6333, 9577, 77, 5236, 6593, 8246, 7518, 5064, 642, 6995, 5525, 4289, 1595, 4662, 4731, 9146, 4043, 8050, 8411, 8185, 3985, 2837, 7087, 1950, 7912, 6896, 6398, 6275, 2051, 3488, 2311, 8225, 3600, 3002, 9089, 9615, 8664, 1018, 8897, 6234, 6806, 4081, 9598, 7213, 6181, 705, 394, 1618, 347, 7793, 4608, 7602, 5636, 3988, 5049, 3004, 6924, 5280, 6092, 1656, 7878, 2991, 9848, 7296, 7112, 6365, 7648, 3940, 3588, 9760, 7251, 6205, 9822, 447, 9163, 9115, 2855, 7120, 2236, 9448, 591, 6108, 4265, 3098, 3522, 4398, 4338, 870, 8727, 7630, 2416, 4048, 7381, 4225, 1214, 7262, 5538, 8622, 7606, 7736, 9863, 9400, 3305, 126, 5660, 1494, 3743, 5434, 7050, 9564, 6048, 9298, 3921, 4968, 1404, 9508, 2096, 856, 8576, 4220, 7188, 2457, 477, 4625, 747, 4055, 3694, 3236, 6164, 6938, 5747, 2981, 5162, 8252, 3740, 4932, 2196, 9426, 9904, 109, 5922, 5099, 1551, 8907, 4232, 4184, 9000, 99, 2958, 730, 7864, 848, 5871, 5804, 1002, 3263, 9165, 9902, 1593, 5704, 9418, 4769, 7239, 9859, 4847, 9119, 1819, 6416, 6019, 3310, 1784, 4876, 5542, 8113, 7473, 344, 1643, 3461, 5347, 5159, 1955, 8947, 589, 9771, 2504, 1884, 216, 2149, 871, 801, 6413, 7943, 7330, 4560, 7475, 7196, 5981, 2305, 4777, 860, 9414, 1399, 3430, 1496, 2561, 6689, 4915, 2908, 8833, 3369, 8955, 8319, 1833, 4877, 7249, 148, 4688, 4092, 3998, 6539, 4969, 1647, 3966, 1381, 4296, 5314, 6039, 5326, 843, 896, 5832, 1216, 2544, 7701, 6473, 2722, 6392, 6489, 7268, 3502, 5932, 6509, 2604, 2700, 8450, 7447, 8788, 1564, 4278, 3160, 9466, 4079, 6285, 1761, 5419, 4064, 4776, 9838, 3786, 104, 2299, 4991, 932, 6236, 9527, 4617, 3325, 7929, 5941, 8322, 4727, 2910, 2372, 2820, 5607, 5661, 9488, 4887, 7107, 8163, 9120, 6468, 6228, 190, 5176, 6745, 5900, 6606, 2015, 5005, 3515, 6334, 8754, 4136, 4787, 8781, 2934, 3862, 909, 9128, 4535, 5905, 757, 6573, 127, 8666, 1375, 6563, 7571, 3480, 4363, 929, 6902, 2798, 7456, 6831, 9750, 5379, 865, 9670, 9836, 5393, 9230, 93, 9443, 781, 6245, 3438, 2992, 7796, 9735, 1268, 1527, 3738, 895, 3517, 5447, 4365, 6319, 696, 6369, 4916, 188, 265, 8957, 7953, 9464, 2469, 6451, 5999, 9559, 1032, 6854, 8211, 9736, 9755, 4201, 867, 9923, 8303, 7365, 2523, 9725, 4957, 3005, 8308, 8334, 7579, 7115, 8518, 6377, 1918, 4687, 4567, 7044, 4442, 5554, 1240, 7109, 6362, 1286, 1693, 8, 3886, 6322, 3073, 3701, 209, 5220, 2580, 2499, 4268, 7375, 7082, 304, 5523, 8595, 3378, 7823, 8601, 3635, 926, 5545, 1164, 5238, 8786, 4578, 7233, 6132, 302, 9850, 3734, 9324, 4045, 5125, 382, 105, 2518, 1518, 6566, 5062, 8102, 9654, 2248, 2277, 4461, 7567, 529, 4359, 5155, 3061, 2094, 1483, 5387, 7246, 9392, 1034, 4387, 1283, 7916, 4529, 7913, 2192, 3009, 9396, 75, 4914, 5611]\n",
    "EventDFTrans = pd.DataFrame()\n",
    "EventDFTrans['Date'] = events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nodes amount:  108\n",
      "communities numbers:  ['3', '1', '0', '2']\n",
      "detecting sub-communities in community  3\n",
      "nodes amount:  21\n",
      "detecting sub-communities in community  1\n",
      "nodes amount:  24\n",
      "detecting sub-communities in community  0\n",
      "nodes amount:  23\n",
      "detecting sub-communities in community  2\n",
      "nodes amount:  40\n",
      "Raw shape:  (116640000, 5)\n"
     ]
    }
   ],
   "source": [
    "f = open(RecordWritingPath+'F1ScoreSubCommunity.txt', 'w+')\n",
    "f.write('Sub Community Detection\\n')\n",
    "f.write('Taipei Synthetic Global Events'+'\\n')\n",
    "\n",
    "df = pd.read_csv('TaipeiAnnuallySynthetic.csv')\n",
    "# lis_event,df_finalEvents = getEvents(EventDataPath,city)\n",
    "df_finalEvents = EventDFTrans\n",
    "G = makeGraphfromDf(df)\n",
    "partition = getComboPartition(G,comboPath,city)\n",
    "SubPartition,df = getSubCommunity(partition,df)\n",
    "datewiseDF = pd.read_csv('TaipeiDateWiseSynthetic.csv')\n",
    "data = aggregateByCommunities(partition,SubPartition,data=datewiseDF)\n",
    "dataTs = getTimeSeries(data)\n",
    "matrix = dataTs.values\n",
    "matrix = np.log(matrix+1)\n",
    "for i in range(matrix.shape[1]):\n",
    "    matrix[:, i] = (matrix[:, i] - matrix[:, i].min()) / (matrix[:, i].max() - matrix[:, i].min())\n",
    "date = dataTs.index.to_frame().rename(columns={'date':'Date'})\n",
    "result = {}\n",
    "EventsDF = df_finalEvents['Date'].drop_duplicates().to_frame()\n",
    "EventsDF['Anomaly'] = True\n",
    "df = EventsDF.merge(date,on='Date',how='right')\n",
    "for thres in range(1,10, 1):\n",
    "    th = thres/10\n",
    "    print(\"Threshhold: \",th)\n",
    "    outliers = anomalyDetection(matrix,pval = th)\n",
    "    df['outlier'] = outliers\n",
    "    TP = len(df[(df['outlier']==True)&(df['Anomaly']==True)])\n",
    "    FP = len(df[(df['outlier']==False)&(df['Anomaly']==True)])\n",
    "    TN = len(df[(df['outlier']==False)&(df['Anomaly']==False)])\n",
    "    FN = len(df[(df['outlier']==False)&(df['Anomaly']==True)])\n",
    "\n",
    "    Precision = len(df[(df['outlier']==True)&(df['Anomaly']==True)])/len(df[df['outlier']==True])\n",
    "    Recall = len(df[(df['outlier']==True)&(df['Anomaly']==True)])/len(EventsDF)\n",
    "    if Precision+Recall > 0:\n",
    "        F1 = (2*Precision*Recall)/(Precision+Recall)\n",
    "    else:\n",
    "        F1 = 0\n",
    "    result[th] = F1\n",
    "F1df = pd.DataFrame.from_dict(data)\n",
    "F1df['city'] = city\n",
    "F1df.to_csv(RecordWritingPath+'F1scoreAllRecords.csv', mode='a+',hearder=False)\n",
    "bestTh = max(result.items(), key=operator.itemgetter(1))[0]\n",
    "\n",
    "#use best th to calculate again\n",
    "outliers = anomalyDetection(matrix,pval = bestTh)\n",
    "df['outlier'] = outliers\n",
    "TP = len(df[(df['outlier']==True)&(df['Anomaly']==True)])\n",
    "FP = len(df[(df['outlier']==False)&(df['Anomaly']==True)])\n",
    "TN = len(df[(df['outlier']==False)&(df['Anomaly']==False)])\n",
    "FN = len(df[(df['outlier']==False)&(df['Anomaly']==True)])\n",
    "\n",
    "f.wrtie('TP: '+str(TP)+' FP: '+str(FP)+' TN: '+str(TN)+' FN: '+str(FN)+'\\n')\n",
    "f.write('overall F1: '+str(result[bestTh])+', Threshold: '+str(bestTh)+'\\n')\n",
    "f.write('Events amount: '+str(len(EventDFTrans))+' Days: '+str(len(datewiseDF))+' ratio: '+str(len(EventDFTrans)/len(datewiseDF)))\n",
    "EventType = list(df_finalEvents['Type'].unique())\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
